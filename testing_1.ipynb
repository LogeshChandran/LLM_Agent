{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain db-sqlite3 chromadb llama_index beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "import sqlite3\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "class ScrapingAgent:\n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "    \n",
    "    def scrape_product(self, product_url):\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(product_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract product details (modify as per Amazon's structure)\n",
    "        title = soup.find(\"span\", {\"id\": \"productTitle\"}).get_text(strip=True)\n",
    "        price = soup.find(\"span\", {\"class\": \"a-price-whole\"}).get_text(strip=True)\n",
    "\n",
    "        return {\"title\": title, \"price\": price, \"url\": product_url}\n",
    "    \n",
    "    def save_to_db(self, product_data):\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create table if not exists\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS products (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                title TEXT,\n",
    "                price TEXT,\n",
    "                url TEXT,\n",
    "                last_updated TEXT\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Insert or update product\n",
    "        cursor.execute('''\n",
    "            INSERT OR REPLACE INTO products (id, title, price, url, last_updated)\n",
    "            VALUES ((SELECT id FROM products WHERE url = ?), ?, ?, ?, ?)\n",
    "        ''', (product_data[\"url\"], product_data[\"title\"], product_data[\"price\"], product_data[\"url\"], datetime.now()))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def run(self, product_url):\n",
    "        product_data = self.scrape_product(product_url)\n",
    "        self.save_to_db(product_data)\n",
    "        return product_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryAgent:\n",
    "    def __init__(self, db_path, scraping_agent):\n",
    "        self.db_path = db_path\n",
    "        self.scraping_agent = scraping_agent\n",
    "    \n",
    "    def query_product(self, product_url):\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        cursor.execute(\"SELECT * FROM products WHERE url = ?\", (product_url,))\n",
    "        product = cursor.fetchone()\n",
    "        conn.close()\n",
    "\n",
    "        if product:\n",
    "            return {\"title\": product[1], \"price\": product[2], \"url\": product[3], \"last_updated\": product[4]}\n",
    "        else:\n",
    "            # Trigger Scraping Agent if product is not in DB\n",
    "            return self.scraping_agent.run(product_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionAgent:\n",
    "    def __init__(self, db_path):\n",
    "        self.db_path = db_path\n",
    "    \n",
    "    def predict_price_trend(self, product_url):\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        cursor.execute(\"SELECT price, last_updated FROM products WHERE url = ?\", (product_url,))\n",
    "        data = cursor.fetchall()\n",
    "        conn.close()\n",
    "\n",
    "        # Example: Simple trend analysis (implement ML model here)\n",
    "        if len(data) >= 2:\n",
    "            recent_price = float(data[-1][0].replace(\",\", \"\"))\n",
    "            previous_price = float(data[-2][0].replace(\",\", \"\"))\n",
    "            trend = \"increasing\" if recent_price > previous_price else \"decreasing\"\n",
    "            return f\"The price trend is {trend}.\"\n",
    "        else:\n",
    "            return \"Not enough data for trend prediction.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class NotificationAgent:\n",
    "    def __init__(self, telegram_bot_token, chat_id):\n",
    "        self.bot_token = telegram_bot_token\n",
    "        self.chat_id = chat_id\n",
    "    \n",
    "    def send_notification(self, message):\n",
    "        # url = f\"https://api.telegram.org/bot{self.bot_token}/sendMessage\"\n",
    "        url = \"\"\n",
    "        data = {\"chat_id\": self.chat_id, \"text\": message}\n",
    "        requests.post(url, data=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "from chromadb import Client\n",
    "\n",
    "class RecommendationAgent:\n",
    "    def __init__(self):\n",
    "        self.client = Client()\n",
    "        self.embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction()\n",
    "    \n",
    "    def recommend(self, query):\n",
    "        # Example: Dummy recommendations (implement real embeddings)\n",
    "        return [\"Recommended Product 1\", \"Recommended Product 2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    DB_PATH = \"products.db\"\n",
    "    TELEGRAM_BOT_TOKEN = \"your_bot_token\"\n",
    "    CHAT_ID = \"your_chat_id\"\n",
    "    \n",
    "    # Initialize agents\n",
    "    scraper = ScrapingAgent(DB_PATH)\n",
    "    query = QueryAgent(DB_PATH, scraper)\n",
    "    predictor = PredictionAgent(DB_PATH)\n",
    "    notifier = NotificationAgent(TELEGRAM_BOT_TOKEN, CHAT_ID)\n",
    "    recommender = RecommendationAgent()\n",
    "    \n",
    "    # Example product URL\n",
    "    # product_url = \"https://www.amazon.in/dp/example-product-id\"\n",
    "    product_url = \"https://www.amazon.in/boAt-BassHeads-100-Headphones-Black/dp/B071Z8M4KX\"\n",
    "    \n",
    "    # Run agents\n",
    "    scraper.run(product_url)\n",
    "    product_data = query.query_product(product_url)\n",
    "    trend = predictor.predict_price_trend(product_url)\n",
    "    notifier.send_notification(f\"Product: {product_data['title']} | Price: {product_data['price']} | Trend: {trend}\")\n",
    "    recommendations = recommender.recommend(product_data['title'])\n",
    "    print(\"Recommendations:\", recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Database connection\n",
    "db_path = \"products.db\"  # Path to your SQLite database\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL command to create the products table\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS products (\n",
    "    product_id INTEGER PRIMARY KEY , -- Auto-incrementing unique ID\n",
    "    name TEXT NOT NULL,                          -- Product name\n",
    "    image_urls TEXT,                             -- List of image URLs (comma-separated)\n",
    "    video_urls TEXT,                             -- List of video URLs (comma-separated)\n",
    "    offer_details TEXT,                          -- Offer details as text\n",
    "    description TEXT                             -- Product description\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL command\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "print(\"Table 'products' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "class MarketplaceAgent:\n",
    "    def __init__(self, db_path, llm):\n",
    "        self.db_path = db_path\n",
    "        self.llm = llm\n",
    "\n",
    "    def llm_call(self, prompt):\n",
    "        \"\"\"Call the LLM with a given prompt.\"\"\"\n",
    "        try:\n",
    "            print(prompt)\n",
    "            response = self.llm(prompt)\n",
    "            print(response)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            return f\"Error during LLM call: {e}\"\n",
    "\n",
    "    # def scrape_product(self, product_url):\n",
    "    #     \"\"\"Use LLM to extract product details from a URL.\"\"\"\n",
    "    #     prompt = f\"\"\"\n",
    "    #     You are an expert assistant. Analyze the following Amazon product URL and provide the title, price, and other metadata:\n",
    "\n",
    "    #     URL: {product_url}\n",
    "    #     Response format: {{ \"title\": \"Product Title\", \"price\": \"123.45\", \"url\": \"{product_url}\" }}\n",
    "    #     \"\"\"\n",
    "    #     response = self.llm_call(prompt)\n",
    "    #     try:\n",
    "    #         return eval(response)  # Convert LLM response into a dictionary\n",
    "    #     except Exception as e:\n",
    "    #         return {\"error\": f\"Failed to parse LLM response: {e}\"}\n",
    "\n",
    "    def scrape_product(self, product_url):\n",
    "        \"\"\"Fetch product details by scraping and using the LLM for structured output.\"\"\"\n",
    "        try:\n",
    "            # Retrieve the webpage content\n",
    "            response = requests.get(product_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            # Remove all tags\n",
    "            for tag in soup.find_all(True):  # True matches all tags\n",
    "                tag.decompose()\n",
    "\n",
    "            # Get the text without tags\n",
    "            clean_text = soup.get_text()\n",
    "            # Convert HTML to string for LLM processing\n",
    "            # html_data = str(soup)\n",
    "            class HTMLData(BaseModel):\n",
    "                product_id: str = Field(description=\"Unique identifier for the product\")\n",
    "                product_name: str = Field(description=\"The product's name or title\")\n",
    "                image_urls: list = Field(description=\"URLs of all images associated with the product\")\n",
    "                video_urls: list = Field(description=\"URLs of any videos associated with the product\")\n",
    "                offer_details: str = Field(description=\"Pricing, discounts, or promotions\")\n",
    "                description: str = Field(description=\"Detailed text describing the product\")\n",
    "            \n",
    "            parser = PydanticOutputParser(pydantic_object = HTMLData)\n",
    "            html_data = clean_text.strip()\n",
    "            print(\"Html data: \", html_data)\n",
    "            template = \"\"\"\n",
    "            You are a data extraction model. Your job is to analyze the provided HTML content of an Amazon product page and extract the following details. If any field is not available, return an empty value as specified below.\n",
    "\n",
    "                1. Product ID: The unique identifier for the product. If not found, return an empty string.\n",
    "                2. Name: The product's name or title. If not found, return an empty string.\n",
    "                3. Image URLs: A list of URLs to all images associated with the product. If not found, return an empty list `[]`.\n",
    "                4. Video URLs: A list of URLs to any videos associated with the product. If not found, return an empty list `[]`.\n",
    "                5. Offer Details: The pricing, discounts, or promotions related to the product. If not found, return an empty string.\n",
    "                6. Description: A detailed textual description of the product. If not found, return an empty string.\n",
    "\n",
    "                HTML Content:\n",
    "                {html_data}\n",
    "\n",
    "                format_instructions:\n",
    "                {format_instructions}\n",
    "            \"\"\"\n",
    "            format_instructions = parser.get_format_instructions()\n",
    "            prompt = PromptTemplate(\n",
    "                template=template,\n",
    "                input_type=[\"html_data\"],\n",
    "                partial_variables={\"format_instructions\": format_instructions},\n",
    "            )\n",
    "            # llm_response = self.llm_call(prompt)\n",
    "            # chain = prompt | self.llm_call | parser\n",
    "            # llm_response = chain.invoke({\"html_data\": html_data})\n",
    "            chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "            raw_output = chain.run({\"html_data\": html_data})\n",
    "            llm_response = parser.parse(raw_output)\n",
    "\n",
    "            # Attempt to parse the LLM's response\n",
    "            # try:\n",
    "            return eval(llm_response)\n",
    "            # except Exception as parse_error:\n",
    "            #     print(f\"Response: {llm_response}\")\n",
    "            #     return {\"error\": f\"Parsing LLM response failed: {parse_error}\", \"llm_response\": llm_response}\n",
    "\n",
    "        except requests.exceptions.RequestException as http_error:\n",
    "\n",
    "            return {\"error\": f\"HTTP request failed: {http_error}\"}\n",
    "        except Exception as general_error:\n",
    "            return {\"error\": f\"An error occurred: {general_error}\"}\n",
    "\n",
    "    def query_database(self, sql_query):\n",
    "        \"\"\"Execute the SQL query on the database.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute(sql_query)\n",
    "            result = cursor.fetchall()\n",
    "            conn.close()\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            conn.close()\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def generate_sql(self, user_query):\n",
    "        \"\"\"Convert user query into an SQL statement using LLM.\"\"\"\n",
    "        # Extract database schema\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = cursor.fetchall()\n",
    "        schema = {}\n",
    "\n",
    "        for table in tables:\n",
    "            table_name = table[0]\n",
    "            cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "            schema[table_name] = [col[1] for col in cursor.fetchall()]\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "        # Create LLM prompt for SQL generation\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert SQL assistant. Based on the following database schema, generate a valid SQL query:\n",
    "        \n",
    "        Schema: {schema}\n",
    "        User Query: \"{user_query}\"\n",
    "        SQL Query:\n",
    "        \"\"\"\n",
    "        return self.llm_call(prompt)\n",
    "\n",
    "    def handle_user_query(self, user_query):\n",
    "        \"\"\"Main handler for user queries.\"\"\"\n",
    "        if user_query.startswith(\"http\"):\n",
    "            # Treat as a product URL\n",
    "            print(\"https:\")\n",
    "            return self.scrape_product(user_query)\n",
    "        else:\n",
    "            # Convert question to SQL\n",
    "            sql_query = self.generate_sql(user_query)\n",
    "            return {\"sql_query\": sql_query, \"result\": self.query_database(sql_query)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "\n",
    "class RealLLM:\n",
    "    def __init__(self, model_name):\n",
    "        self.llm = ChatOllama(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_product(product_url):\n",
    "    \"\"\"Fetch product details by scraping and using the LLM for structured output.\"\"\"\n",
    "    try:\n",
    "        USER_AGENTS = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\",\n",
    "        ]\n",
    "\n",
    "        headers = {\n",
    "            \"User-Agent\": random.choice(USER_AGENTS),\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Referer\": \"https://www.google.com/\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "            \"DNT\": \"1\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "        }\n",
    "\n",
    "        # Retrieve the webpage content\n",
    "        response = requests.get(product_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Remove all tags\n",
    "        # for tag in soup.find_all(True):  # True matches all tags\n",
    "        #     tag.decompose()\n",
    "\n",
    "        # Get the text without tags\n",
    "        clean_text = soup.get_text()\n",
    "        # Convert HTML to string for LLM processing\n",
    "        # html_data = str(soup)\n",
    "        class HTMLData(BaseModel):\n",
    "            product_id: str = Field(description=\"Unique identifier for the product\")\n",
    "            product_name: str = Field(description=\"The product's name or title\")\n",
    "            image_urls: list = Field(description=\"URLs of all images associated with the product\")\n",
    "            video_urls: list = Field(description=\"URLs of any videos associated with the product\")\n",
    "            offer_details: str = Field(description=\"any Pricing, discounts, or promotions\")\n",
    "            description: str = Field(description=\"Detailed text describing the product\")\n",
    "        \n",
    "        parser = PydanticOutputParser(pydantic_object = HTMLData)\n",
    "        html_data = clean_text.strip()\n",
    "        print(\"Html data: \", html_data)\n",
    "        template = \"\"\"\n",
    "        You are a data extraction model. Your job is to analyze the provided HTML content of an Amazon product page and extract the following details. If any field is not available, return an empty value as specified below.\n",
    "\n",
    "            1. Product ID: The unique identifier for the product. If not found, return an empty string.\n",
    "            2. Name: The product's name or title. If not found, return an empty string.\n",
    "            3. Image URLs: A list of URLs to all images associated with the product. If not found, return an empty list `[]`.\n",
    "            4. Video URLs: A list of URLs to any videos associated with the product. If not found, return an empty list `[]`.\n",
    "            5. Offer Details: The pricing, discounts, or promotions related to the product. If not found, return an empty string.\n",
    "            6. Description: A detailed textual description of the product. If not found, return an empty string.\n",
    "\n",
    "            **HTML Content**:\n",
    "            {html_data}\n",
    "\n",
    "            Examples:\n",
    "            { \"product_id\": \"12345\", \"product_name\": \"Product Name\", \"image_urls\": [\"url1\", \"url2\"], \"video_urls\": [\"url1\", \"url2\"], \"offer_details\": \"Offer details\", \"description\": \"Product description\" }\n",
    "            \n",
    "            Return only the required fields in the following JSON format:\n",
    "            **Required JSON format**:\n",
    "            format_instructions:\n",
    "            {format_instructions}\n",
    "        \"\"\"\n",
    "        format_instructions = parser.get_format_instructions()\n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_type=[\"html_data\"],\n",
    "            partial_variables={\"format_instructions\": format_instructions},\n",
    "        )\n",
    "        # llm_response = self.llm_call(prompt)\n",
    "        # chain = prompt | self.llm_call | parser\n",
    "        # llm_response = chain.invoke({\"html_data\": html_data})\n",
    "        # chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        chain = prompt | llm\n",
    "        raw_output = chain.invoke({\"html_data\": soup})\n",
    "        print(\"Raw output: \", raw_output)\n",
    "        llm_response = parser.parse(raw_output)\n",
    "\n",
    "        print(\"Response: \", llm_response)\n",
    "        \n",
    "        # Attempt to parse the LLM's response\n",
    "        # try:\n",
    "        return eval(llm_response)\n",
    "        # except Exception as parse_error:\n",
    "        #     print(f\"Response: {llm_response}\")\n",
    "        #     return {\"error\": f\"Parsing LLM response failed: {parse_error}\", \"llm_response\": llm_response}\n",
    "\n",
    "    except requests.exceptions.RequestException as http_error:\n",
    "\n",
    "        return {\"error\": f\"HTTP request failed: {http_error}\"}\n",
    "    except Exception as general_error:\n",
    "        return {\"error\": f\"An error occurred: {general_error}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"deepseek-r1:8b\"\n",
    "model_name = \"llama-3.2\"\n",
    "RealLLM_obj = RealLLM(model_name)\n",
    "llm = RealLLM_obj.llm\n",
    "# Create an instance of MarketplaceAgent\n",
    "# agent = MarketplaceAgent(db_path, RealLLM_obj.llm)\n",
    "\n",
    "# Example 1: Scrape a product URL\n",
    "product_url = \"https://www.amazon.in/Daikin-Inverter-Display-Technology-MTKL50U/dp/B0BK1KS6ZD/?th=1\"\n",
    "product_data = scrape_product(product_url)\n",
    "print(\"Product Data:\", product_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize LLM and Database Path\n",
    "    # llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "    db_path = \"products.db\"  # Path to your SQLite database\n",
    "\n",
    "    # model_name = \"deepseek-r1:8b\"\n",
    "    model_name = \"llama-3.2\"\n",
    "    RealLLM_obj = RealLLM(model_name)\n",
    "    # Create an instance of MarketplaceAgent\n",
    "    agent = MarketplaceAgent(db_path, RealLLM_obj.llm)\n",
    "\n",
    "    # Example 1: Scrape a product URL\n",
    "    product_url = \"https://www.amazon.in/boAt-BassHeads-100-Headphones-Black/dp/B071Z8M4KX?th=1\"\n",
    "    product_data = agent.handle_user_query(product_url)\n",
    "    print(\"Product Data:\", product_data)\n",
    "\n",
    "    # Example 2: User query for database search\n",
    "    user_query = \"Show all products priced below 1000.\"\n",
    "    query_result = agent.handle_user_query(user_query)\n",
    "    print(\"Query Result:\", query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_url = \"https://www.amazon.in/boAt-BassHeads-100-Headphones-Black/dp/B071Z8M4KX?th=1\"\n",
    "# response = requests.get(product_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# # Remove all tags\n",
    "# for tag in soup.find_all(True):  # True matches all tags\n",
    "#     tag.decompose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.get_text().replace(\"\\n\\n\", \"\\n\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "import json\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='scraper_errors.log', level=logging.ERROR)\n",
    "\n",
    "# Initialize LLM\n",
    "class RealLLM:\n",
    "    def __init__(self, model_name):\n",
    "        self.llm = ChatOllama(model=model_name)\n",
    "\n",
    "\n",
    "model_name = \"llama3.2:latest\"\n",
    "RealLLM_obj = RealLLM(model_name)\n",
    "llm = RealLLM_obj.llm\n",
    "\n",
    "# Scrape Product Details Tool\n",
    "# @tool\n",
    "def scrape_product_details(page_url: str):\n",
    "    \"\"\"\n",
    "    Scrapes product details from a product page URL and returns a JSON object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        USER_AGENTS = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\",\n",
    "        ]\n",
    "\n",
    "        headers = {\n",
    "            \"User-Agent\": random.choice(USER_AGENTS),\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Referer\": \"https://www.google.com/\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "            \"DNT\": \"1\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "        }\n",
    "\n",
    "        # Retrieve the webpage content\n",
    "        response = requests.get(product_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        \n",
    "        product_name = soup.find(\"span\", id=\"productTitle\")\n",
    "        price = soup.find(\"span\", class_=\"a-price-whole\")\n",
    "        offer = soup.find(\"span\", class_=\"savingsPercentage\")\n",
    "        rating = soup.find(\"span\", class_=\"a-size-base a-color-base\")\n",
    "        brand = soup.find(\"tr\", class_=\"po-brand\")\n",
    "        category = \"Face Makeup\"  # Assuming category since not explicitly in HTML\n",
    "        description_list = soup.select(\"#feature-bullets ul li span.a-list-item\")\n",
    "        description = soup.find(\"div\", id=\"productDescription\")\n",
    "        image = soup.find(\"img\", id=\"landingImage\")\n",
    "\n",
    "        url_parts = product_url.split(\"/\")\n",
    "        product_id = url_parts[url_parts.index(\"dp\") + 1] \n",
    "        data = {\n",
    "            \"product_id\": product_id,\n",
    "            \"prpduct_url\" : product_url,\n",
    "            \"product_name\": product_name.get_text(strip=True) if product_name else None,\n",
    "            \"price\": f\"₹{price.get_text(strip=True)}\" if price else None,\n",
    "            \"offer\": offer.get_text(strip=True) if offer else None,\n",
    "            \"rating\": float(rating.get_text(strip=True)) if rating else None,\n",
    "            # \"category\": category,\n",
    "            \"brand\": brand.find_all(\"td\")[1].get_text(strip=True) if brand else None,\n",
    "            \"description1\": \", \".join([desc.get_text(strip=True) for desc in description_list]) if description_list else None,\n",
    "            \"product_descripation\" : description.get_text(strip=True) if description else None,\n",
    "            \"image_url\": image[\"src\"] if image else None\n",
    "        }\n",
    "\n",
    "        # print(json.dumps(data, indent=4, ensure_ascii=False))\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}, URL: {product_url}\")\n",
    "        return {\"error\": f\"An error occurred: {e}\"}\n",
    "\n",
    "# JSON Formatter Tool\n",
    "def format_json(data: dict):\n",
    "    \"\"\"\n",
    "    Formats a given dictionary into a properly formatted JSON string.\n",
    "    \"\"\"\n",
    "    return json.dumps(data, indent=4)\n",
    "\n",
    "\n",
    "def create_product_db():\n",
    "    conn = sqlite3.connect(\"products.db\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create table if it doesn't exist\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS products (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        product_id TEXT,\n",
    "        product_url TEXT,\n",
    "        product_name TEXT,\n",
    "        price TEXT,\n",
    "        offer TEXT,\n",
    "        rating REAL,\n",
    "        brand TEXT,\n",
    "        description1 TEXT,\n",
    "        product_description TEXT,\n",
    "        image_url TEXT\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def save_to_db(product_data: dict):\n",
    "    \"\"\"\n",
    "    Saves product details to an SQLite database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(\"products.db\")\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Ensure table exists before inserting data\n",
    "        create_product_db()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "        INSERT INTO products (product_id, product_url, product_name, price, offer, rating, brand, description1, product_description, image_url)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            product_data.get(\"product_id\"),\n",
    "            product_data.get(\"product_url\"),\n",
    "            product_data.get(\"product_name\"),\n",
    "            product_data.get(\"price\"),\n",
    "            product_data.get(\"offer\"),\n",
    "            product_data.get(\"rating\"),\n",
    "            product_data.get(\"brand\"),\n",
    "            product_data.get(\"description1\"),\n",
    "            product_data.get(\"product_descripation\"),\n",
    "            product_data.get(\"image_url\")\n",
    "        ))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        return {\"status\": \"Product details saved successfully!\"}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}, URL: {product_url}\")\n",
    "        return {\"error\": f\"Database error: {e}\"}\n",
    "\n",
    "\n",
    "# Define LangChain Tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"ProductScraper\",\n",
    "        func=scrape_product_details,\n",
    "        description=\"Scrapes product details from a given URL and returns a JSON object.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"JSONFormatter\",\n",
    "        func=format_json,\n",
    "        description=\"Formats a given dictionary into a properly formatted JSON string.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"DatabaseSaver\",\n",
    "        func=save_to_db,\n",
    "        description=\"Saves product details into a database.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# 🔹 Initialize the Agent with AUTO Tool Selection\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Ensures stepwise tool selection\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "product_url = \"https://www.amazon.in/LAKM%C3%89-01-Beige-Coverage-30-Tinted-Moisturizer/dp/B01BBNF6C6\"\n",
    "result = agent.run(f\"Get details about the product at this URL: {product_url}. Format as JSON and save to the database.\")\n",
    "\n",
    "# Print Final JSON Output\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_product_details(product_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_url = \"https://www.amazon.in/boAt-BassHeads-100-Headphones-Black/dp/B071Z8M4KX\"\n",
    "product_url = \"https://www.amazon.in/RENEE-Lumi-Glow-Highlighting-Pigmentation/dp/B0D22KD4VC/\"\n",
    "product_url = \"https://www.amazon.in/LAKM%C3%89-01-Beige-Coverage-30-Tinted-Moisturizer/dp/B01BBNF6C6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENTS = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\",\n",
    "        ]\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": random.choice(USER_AGENTS),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "\n",
    "# Retrieve the webpage content\n",
    "response = requests.get(product_url, headers=headers)\n",
    "response.raise_for_status()\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Extracting product details\n",
    "product_name = soup.find(\"span\", id=\"productTitle\")\n",
    "price = soup.find(\"span\", class_=\"a-price-whole\")\n",
    "offer = soup.find(\"span\", class_=\"savingsPercentage\")\n",
    "rating = soup.find(\"span\", class_=\"a-size-base a-color-base\")\n",
    "brand = soup.find(\"tr\", class_=\"po-brand\")\n",
    "category = \"Face Makeup\"  # Assuming category since not explicitly in HTML\n",
    "description_list = soup.select(\"#feature-bullets ul li span.a-list-item\")\n",
    "description = soup.find(\"div\", id=\"productDescription\")\n",
    "image = soup.find(\"img\", id=\"landingImage\")\n",
    "\n",
    "url_parts = product_url.split(\"/\")\n",
    "product_id = url_parts[url_parts.index(\"dp\") + 1] \n",
    "data = {\n",
    "    \"product_id\": product_id,\n",
    "    \"prpduct_url\" : product_url,\n",
    "    \"product_name\": product_name.get_text(strip=True) if product_name else None,\n",
    "    \"price\": f\"₹{price.get_text(strip=True)}\" if price else None,\n",
    "    \"offer\": offer.get_text(strip=True) if offer else None,\n",
    "    \"rating\": float(rating.get_text(strip=True)) if rating else None,\n",
    "    # \"category\": category,\n",
    "    \"brand\": brand.find_all(\"td\")[1].get_text(strip=True) if brand else None,\n",
    "    \"description1\": \", \".join([desc.get_text(strip=True) for desc in description_list]) if description_list else None,\n",
    "    \"product_descripation\" : description.get_text(strip=True) if description else None,\n",
    "    \"image_url\": image[\"src\"] if image else None\n",
    "}\n",
    "\n",
    "print(json.dumps(data, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.tools import Tool\n",
    "from langchain.schema import AgentFinish\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from langgraph.graph import Graph, END\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='scraper_errors.log', level=logging.ERROR)\n",
    "\n",
    "# Initialize LLM\n",
    "class RealLLM:\n",
    "    def __init__(self, model_name):\n",
    "        self.llm = ChatOllama(model=model_name)\n",
    "\n",
    "\n",
    "model_name = \"llama3.2:latest\"\n",
    "RealLLM_obj = RealLLM(model_name)\n",
    "llm = RealLLM_obj.llm\n",
    "\n",
    "# 🟢 Step 1: Define State Management\n",
    "class ScraperState(TypedDict):\n",
    "    homepage_url: str\n",
    "    product_urls: Optional[List[str]] = []\n",
    "    current_product: Optional[str] = None\n",
    "    status: str = \"INITIAL\"  # Possible states: INITIAL, SCRAPING_URLS, SCRAPING_PRODUCTS, SAVING_TO_DB, DONE\n",
    "    errors: Optional[List[str]] = []\n",
    "\n",
    "state = ScraperState(homepage_url=\"https://www.amazon.in/s?k=laptops\")\n",
    "\n",
    "# 🟢 Step 2: User-Agent Headers\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "]\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": random.choice(USER_AGENTS),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "}\n",
    "\n",
    "# 🟢 Step 3: Scrape All Product URLs from Homepage\n",
    "def scrape_product_urls(state: ScraperState):\n",
    "    try:\n",
    "        response = requests.get(state.homepage_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        product_links = []\n",
    "        for a_tag in soup.select(\"a[href*='/dp/']\"):\n",
    "            href = a_tag[\"href\"]\n",
    "            url_parts = href.split(\"/\")\n",
    "            product_id = url_parts[url_parts.index(\"dp\") + 1] if \"dp\" in url_parts else None\n",
    "            product_url = f\"https://www.amazon.in/dp/{product_id}/\"\n",
    "            product_links.append(product_url)\n",
    "\n",
    "        product_links = list(set(product_links))  # Remove duplicates\n",
    "        state.product_urls = product_links\n",
    "        state.status = \"SCRAPING_PRODUCTS\"\n",
    "        # return f\"Scraped {len(product_links)} product URLs.\"\n",
    "        return state\n",
    "    except Exception as e:\n",
    "        state.errors.append(str(e))\n",
    "        state.status = \"DONE\"\n",
    "        # return f\"Error scraping homepage: {e}\"\n",
    "        return state\n",
    "\n",
    "# 🟢 Step 4: Scrape Product Details\n",
    "def scrape_product_details(state: ScraperState):\n",
    "    if not state.product_urls:\n",
    "        # return \"No product URLs found.\"\n",
    "        state.status = \"DONE\"\n",
    "        return state\n",
    "\n",
    "    product_url = state.product_urls.pop(0)  # Process one URL at a time\n",
    "    state.current_product = product_url\n",
    "\n",
    "    try:\n",
    "        response = requests.get(product_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        product_name = soup.find(\"span\", id=\"productTitle\")\n",
    "        price = soup.find(\"span\", class_=\"a-price-whole\")\n",
    "        offer = soup.find(\"span\", class_=\"savingsPercentage\")\n",
    "        rating = soup.find(\"span\", class_=\"a-size-base a-color-base\")\n",
    "        brand = soup.find(\"tr\", class_=\"po-brand\")\n",
    "        description_list = soup.select(\"#feature-bullets ul li span.a-list-item\")\n",
    "        description = soup.find(\"div\", id=\"productDescription\")\n",
    "        image = soup.find(\"img\", id=\"landingImage\")\n",
    "\n",
    "        product_id = product_url.split(\"/dp/\")[1].split(\"/\")[0]\n",
    "\n",
    "        product_data = {\n",
    "            \"product_id\": product_id,\n",
    "            \"product_url\": product_url,\n",
    "            \"product_name\": product_name.get_text(strip=True) if product_name else None,\n",
    "            \"price\": f\"₹{price.get_text(strip=True)}\" if price else None,\n",
    "            \"offer\": offer.get_text(strip=True) if offer else None,\n",
    "            \"rating\": float(rating.get_text(strip=True)) if rating else None,\n",
    "            \"brand\": brand.find_all(\"td\")[1].get_text(strip=True) if brand else None,\n",
    "            \"description1\": \", \".join([desc.get_text(strip=True) for desc in description_list]) if description_list else None,\n",
    "            \"product_description\": description.get_text(strip=True) if description else None,\n",
    "            \"image_url\": image[\"src\"] if image else None,\n",
    "        }\n",
    "\n",
    "        state.status = \"SAVING_TO_DB\"\n",
    "        return product_data\n",
    "    except Exception as e:\n",
    "        state.errors.append(str(e))\n",
    "        # return {\"error\": f\"Error scraping product: {e}\"}\n",
    "        return state\n",
    "\n",
    "# 🟢 Step 5: Save Data to SQLite Database\n",
    "def save_to_db(state: ScraperState, product_data: dict):\n",
    "    try:\n",
    "        conn = sqlite3.connect(\"products.db\")\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS products (\n",
    "                product_id TEXT PRIMARY KEY,\n",
    "                product_url TEXT,\n",
    "                product_name TEXT,\n",
    "                price TEXT,\n",
    "                offer TEXT,\n",
    "                rating REAL,\n",
    "                brand TEXT,\n",
    "                description1 TEXT,\n",
    "                product_description TEXT,\n",
    "                image_url TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO products (product_id, product_url, product_name, price, offer, rating, brand, description1, product_description, image_url)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ON CONFLICT(product_id) DO UPDATE SET \n",
    "                product_url = excluded.product_url,\n",
    "                product_name = excluded.product_name,\n",
    "                price = excluded.price,\n",
    "                offer = excluded.offer,\n",
    "                rating = excluded.rating,\n",
    "                brand = excluded.brand,\n",
    "                description1 = excluded.description1,\n",
    "                product_description = excluded.product_description,\n",
    "                image_url = excluded.image_url\n",
    "        \"\"\", (\n",
    "            product_data.get(\"product_id\"),\n",
    "            product_data.get(\"product_url\"),\n",
    "            product_data.get(\"product_name\"),\n",
    "            product_data.get(\"price\"),\n",
    "            product_data.get(\"offer\"),\n",
    "            product_data.get(\"rating\"),\n",
    "            product_data.get(\"brand\"),\n",
    "            product_data.get(\"description1\"),\n",
    "            product_data.get(\"product_description\"),\n",
    "            product_data.get(\"image_url\"),\n",
    "        ))\n",
    "\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        state.status = \"SCRAPING_PRODUCTS\" if state.product_urls else \"DONE\"\n",
    "        return f\"Product {product_data['product_id']} saved!\"\n",
    "    except Exception as e:\n",
    "        state.errors.append(str(e))\n",
    "        # return {\"error\": f\"Database error: {e}\"}\n",
    "        return state\n",
    "\n",
    "# 🟢 Step 6: Define LangChain Tools\n",
    "scrape_urls_tool = Tool(name=\"Scrape Product URLs\", func=scrape_product_urls, description=\"Scrape all product URLs from homepage.\")\n",
    "scrape_details_tool = Tool(name=\"Scrape Product Details\", func=scrape_product_details, description=\"Scrape details of a product from a given URL.\")\n",
    "save_db_tool = Tool(name=\"Save Product to DB\", func=save_to_db, description=\"Save product data to SQLite database.\")\n",
    "\n",
    "# 🟢 Step 7: Initialize AI Agent\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[scrape_urls_tool, scrape_details_tool, save_db_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "# 🟢 Step 8: Execute the Workflow\n",
    "print(\"🔄 Starting Scraping Process...\")\n",
    "agent.invoke(\"Scrape Product URLs\", state)\n",
    "# agent.invoke(\"Scrape Product URLs\", state.dict())\n",
    "while state.status == \"SCRAPING_PRODUCTS\":\n",
    "    product_data = agent.invoke(\"Scrape Product Details\", state)\n",
    "    if \"error\" in product_data:\n",
    "        print(product_data[\"error\"])\n",
    "    else:\n",
    "        agent.invoke(\"Save Product to DB\", state, product_data)\n",
    "\n",
    "print(\"✅ Scraping Completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.workflow import Graph\n",
    "\n",
    "# Define each scraping function as a node\n",
    "workflow = Graph()\n",
    "\n",
    "# Adding nodes for each step in the workflow\n",
    "workflow.set_entry_point(\"scrape_product_urls\")\n",
    "\n",
    "workflow.add_node(\"scrape_product_urls\", scrape_product_urls)\n",
    "workflow.add_node(\"scrape_product_details\", scrape_product_details)\n",
    "workflow.add_node(\"save_to_db\", save_to_db)\n",
    "\n",
    "# Adding edges to define the order of execution\n",
    "workflow.add_edge(\"scrape_product_urls\", \"scrape_product_details\")\n",
    "\n",
    "# Conditional edge for scraping details based on product URLs available\n",
    "workflow.add_conditional_edges(\n",
    "    \"scrape_product_details\",\n",
    "    lambda state: \"save_to_db\" if state[\"product_urls\"] else \"END\",  # Check if we have product URLs to scrape\n",
    "    {\n",
    "        \"save_to_db\": \"save_to_db\",  # Proceed to saving data if products are available\n",
    "        \"END\": END  # End the process if no products are available\n",
    "    }\n",
    ")\n",
    "\n",
    "# Optionally, add more conditional edges or other logic for error handling or retries\n",
    "# Example: If there's an error in scraping, you can add a fallback to retry or log the error\n",
    "workflow.add_conditional_edges(\n",
    "    \"scrape_product_urls\",\n",
    "    lambda state: \"END\" if len(state[\"errors\"]) > 0 else \"scrape_product_details\",\n",
    "    {\n",
    "        \"END\": END,  # End if there are errors\n",
    "        \"scrape_product_details\": \"scrape_product_details\",  # Proceed to next step if no errors\n",
    "    }\n",
    ")\n",
    "\n",
    "# Optionally, add more nodes or conditional logic\n",
    "# workflow.add_node(\"error_handling\", error_handling_function)\n",
    "\n",
    "# Final edge to indicate completion\n",
    "workflow.add_edge(\"save_to_db\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    IPImage(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the workflow diagram as a PNG file\n",
    "workflow_diagram = app.get_graph().draw_mermaid_png()\n",
    "\n",
    "# Save the image file\n",
    "with open(\"workflow_diagram.png\", \"wb\") as f:\n",
    "    f.write(workflow_diagram)\n",
    "\n",
    "print(\"Workflow diagram saved as workflow_diagram.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_workflow(query: str):\n",
    "    \"\"\"Run the LangGraph workflow and display results.\"\"\"\n",
    "    initial_state = {\n",
    "       \"homepage_url\": query,\n",
    "        \"product_urls\": [],\n",
    "        \"current_product\": None,\n",
    "        \"status\": \"INITIAL\",\n",
    "        \"errors\": [],\n",
    "    }\n",
    "    try:\n",
    "        result = await app.ainvoke(initial_state)\n",
    "        \n",
    "        return result[\"formatted_results\"]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"https://www.amazon.in/s?srs=93797680031&bbn=93797680031&rh=p_72%3A1318476031&dc&ds=v1%3AtSB1tQ%2BflV1BQc6XbzK6p1izuWr0xV2adbDC6Zy6FeE&qid=1738386643&rnid=1318475031&ref=sr_nr_p_72_1\"\n",
    "print(await run_workflow(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_product_urls(homepage_url):\n",
    "    try:\n",
    "        USER_AGENTS = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "        ]\n",
    "\n",
    "        headers = {\n",
    "            \"User-Agent\": random.choice(USER_AGENTS),\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Referer\": \"https://www.google.com/\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "            \"DNT\": \"1\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "        }\n",
    "\n",
    "        response = requests.get(homepage_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        product_links = []\n",
    "        for a_tag in soup.select(\"a[href*='/dp/']\"):\n",
    "            href = a_tag[\"href\"]\n",
    "            url_parts = href.split(\"/\")\n",
    "            product_id = url_parts[url_parts.index(\"dp\") + 1] if \"dp\" in url_parts else None\n",
    "            product_url = f\"https://www.amazon.in/dp/{product_id}/\"\n",
    "            product_links.append(product_url)\n",
    "\n",
    "        product_links = list(set(product_links))  # Remove duplicates\n",
    "        state.product_urls = product_links\n",
    "        state.status = \"SCRAPING_PRODUCTS\"\n",
    "        return f\"Scraped {len(product_links)} product URLs.\"\n",
    "    except Exception as e:\n",
    "        state.errors.append(str(e))\n",
    "        state.status = \"DONE\"\n",
    "        return f\"Error scraping homepage: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "homepage_url = \"https://www.amazon.in/s?k=cosmetics+for+women&i=beauty&rh=n%3A1355016031&dc=&crid=ACTZFOKZB8Q6&nsdOptOutParam=true&qid=1738383717&rnid=1741387031&sprefix=cos%2Cbeauty%2C697&ref=sr_nr_p_36_0_0&low-price=&high-price=10000\"\n",
    "meg,product_url = scrape_product_urls(homepage_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"products.db\")\n",
    "\n",
    "query = \"SELECT * FROM products\"\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(query)\n",
    "result = cursor.fetchall()\n",
    "conn.close()\n",
    "\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete table\n",
    "conn = sqlite3.connect(\"products.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"DROP TABLE products\")\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "conn = sqlite3.connect(\"products.db\")\n",
    "query = \"SELECT * FROM products\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "df.to_excel('products_data.xlsx', index=False)\n",
    "\n",
    "print(\"Data has been saved to 'products_data.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasai.llm.local_llm import LocalLLM\n",
    "import pandas as pd\n",
    "from pandasai import SmartDataframe\n",
    "from pandas_ai \n",
    "\n",
    "model = LocalLLM(\n",
    "    api_base=\"http://localhost:11434/v1\",\n",
    "    # model=\"llama3\"\n",
    "    model=\"llama3.2:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_file = \"products_data 2.xlsx\"\n",
    "data =  pd.read_excel(uploaded_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=SmartDataframe(data,config={\"llm\":model})\n",
    "# prompt = \"iphone best offer list\"\n",
    "# df.chat(query=prompt,output_type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "\n",
    "import pandas as pd\n",
    "# from langchain_openai import OpenAI\n",
    "\n",
    "model_name = \"llama3.2:latest\"\n",
    "RealLLM_obj = RealLLM(model_name)\n",
    "llm = RealLLM_obj.llm\n",
    "df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_pandas_dataframe_agent(llm, df, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph\n",
    "dot = Digraph(comment=\"CrewAI Multi-Agent System\")\n",
    "\n",
    "# Define agents as nodes\n",
    "dot.node('U', 'User Agent', shape='ellipse', style='filled', fillcolor='lightblue')\n",
    "dot.node('W', 'Web Scraping Agent', shape='box', style='filled', fillcolor='lightgreen')\n",
    "dot.node('D', 'Database Agent', shape='box', style='filled', fillcolor='lightyellow')\n",
    "dot.node('S', 'Social Media Agent', shape='box', style='filled', fillcolor='lightpink')\n",
    "\n",
    "# Define agent relationships\n",
    "dot.edge('U', 'W', label=\"Scrape website data\")\n",
    "dot.edge('U', 'D', label=\"Query database\")\n",
    "dot.edge('U', 'S', label=\"Post to Twitter\")\n",
    "dot.edge('W', 'D', label=\"Store data in DB\")\n",
    "dot.edge('D', 'S', label=\"Retrieve data for social media\")\n",
    "\n",
    "# Save and render the graph\n",
    "dot.render('crewai_agents_flow', format='png', cleanup=True)\n",
    "\n",
    "# Show the graph\n",
    "dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def perform_task(self):\n",
    "        pass  # Define the core function that all agents should execute\n",
    "\n",
    "class SupervisorAgent(BaseAgent):\n",
    "    def __init__(self, name, agents):\n",
    "        super().__init__(name)\n",
    "        self.agents = agents  # List of the 4 agents managed by the supervisor\n",
    "\n",
    "    def manage_agents(self):\n",
    "        for agent in self.agents:\n",
    "            print(f\"Managing agent: {agent.name}\")\n",
    "            agent.perform_task()  # Supervised task execution\n",
    "\n",
    "    def add_agent(self, agent):\n",
    "        self.agents.append(agent)\n",
    "    \n",
    "    def remove_agent(self, agent):\n",
    "        self.agents.remove(agent)\n",
    "\n",
    "class Agent1(BaseAgent):\n",
    "    def perform_task(self):\n",
    "        print(f\"{self.name} is performing task 1.\")\n",
    "        \n",
    "class Agent2(BaseAgent):\n",
    "    def perform_task(self):\n",
    "        print(f\"{self.name} is performing task 2.\")\n",
    "        \n",
    "class Agent3(BaseAgent):\n",
    "    def perform_task(self):\n",
    "        print(f\"{self.name} is performing task 3.\")\n",
    "        \n",
    "class Agent4(BaseAgent):\n",
    "    def perform_task(self):\n",
    "        print(f\"{self.name} is performing task 4.\")\n",
    "\n",
    "# Creating the 4 agents\n",
    "agent1 = Agent1(name=\"Agent1\")\n",
    "agent2 = Agent2(name=\"Agent2\")\n",
    "agent3 = Agent3(name=\"Agent3\")\n",
    "agent4 = Agent4(name=\"Agent4\")\n",
    "\n",
    "# Adding the 4 agents to the supervisor\n",
    "supervisor = SupervisorAgent(name=\"SupervisorAgent\", agents=[agent1, agent2, agent3, agent4])\n",
    "\n",
    "# Supervisor manages the agents\n",
    "supervisor.manage_agents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph\n",
    "dot = Digraph(comment='CrewAI Agent Structure')\n",
    "\n",
    "# Add nodes for SupervisorAgent and the 4 agents\n",
    "dot.node('S', 'SupervisorAgent')\n",
    "dot.node('A1', 'Agent1')\n",
    "dot.node('A2', 'Agent2')\n",
    "dot.node('A3', 'Agent3')\n",
    "dot.node('A4', 'Agent4')\n",
    "\n",
    "# Add edges to show the relationships (Supervisor -> Agents)\n",
    "dot.edge('S', 'A1')\n",
    "dot.edge('S', 'A2')\n",
    "dot.edge('S', 'A3')\n",
    "dot.edge('S', 'A4')\n",
    "\n",
    "# Render and view the graph (optional: to a file, for example, 'agent_structure.pdf')\n",
    "dot.render('agent_structure', format='png', cleanup=True)\n",
    "\n",
    "# Show the graph\n",
    "dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    def __init__(self, name, tools, description):\n",
    "        self.name = name\n",
    "        self.tools = tools  # List of tools available for the agent\n",
    "        self.description = description  # Agent description\n",
    "    \n",
    "    def perform_task(self):\n",
    "        pass  # Define the core function that all agents should execute\n",
    "    \n",
    "    def display_description(self):\n",
    "        print(f\"{self.name}: {self.description}\")\n",
    "    \n",
    "    def display_tools(self):\n",
    "        print(f\"{self.name}'s Tools:\")\n",
    "        for tool in self.tools:\n",
    "            print(f\"- {tool.name}: {tool.description}\")\n",
    "            \n",
    "class Tool:\n",
    "    def __init__(self, name, description):\n",
    "        self.name = name\n",
    "        self.description = description  # Tool description\n",
    "    \n",
    "    def perform_tool_task(self):\n",
    "        pass  # Define the core function of the tool\n",
    "    \n",
    "    def display_description(self):\n",
    "        print(f\"{self.name}: {self.description}\")\n",
    "\n",
    "# Define tools\n",
    "tool1_A = Tool(\"Tool1_A\", \"This tool handles data analysis.\")\n",
    "tool2_A = Tool(\"Tool2_A\", \"This tool performs data cleanup.\")\n",
    "tool3_A = Tool(\"Tool3_A\", \"This tool visualizes results.\")\n",
    "tool4_A = Tool(\"Tool4_A\", \"This tool connects to external APIs.\")\n",
    "tool5_A = Tool(\"Tool5_A\", \"This tool performs reporting.\")\n",
    "\n",
    "# Define Agent1 with description and tools\n",
    "agent1 = BaseAgent(\n",
    "    \"Agent1\", \n",
    "    [tool1_A, tool2_A, tool3_A, tool4_A, tool5_A], \n",
    "    \"Agent1 is responsible for data analysis tasks.\"\n",
    ")\n",
    "\n",
    "# Similarly for Agent2, Agent3, and Agent4 (define other tools and descriptions)\n",
    "tool1_B = Tool(\"Tool1_B\", \"Tool for performance monitoring.\")\n",
    "tool2_B = Tool(\"Tool2_B\", \"Tool for error handling.\")\n",
    "tool3_B = Tool(\"Tool3_B\", \"Tool for cloud synchronization.\")\n",
    "tool4_B = Tool(\"Tool4_B\", \"Tool for system diagnostics.\")\n",
    "tool5_B = Tool(\"Tool5_B\", \"Tool for backup management.\")\n",
    "\n",
    "agent2 = BaseAgent(\n",
    "    \"Agent2\", \n",
    "    [tool1_B, tool2_B, tool3_B, tool4_B, tool5_B], \n",
    "    \"Agent2 handles system health and maintenance tasks.\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph\n",
    "dot = Digraph('CrewAI_Agent_Structure')\n",
    "\n",
    "# Set the colors\n",
    "supervisor_color = '#dff0d8'  # Light Green\n",
    "agent_color = '#ffcc00'  # Yellow\n",
    "tool_color = '#a2d5f2'  # Light Blue\n",
    "\n",
    "# Supervisor Node\n",
    "dot.node('S', 'Supervisor Agent\\n(Manages and supervises all agents)', style='filled', fillcolor=supervisor_color, shape='box')\n",
    "\n",
    "# Subgraph for Agents\n",
    "with dot.subgraph(name='cluster_agents') as agents:\n",
    "    agents.attr(label='Agents', color='black')\n",
    "    agents.node('A1', 'Agent 1\\n(Data Analysis)', style='filled', fillcolor=agent_color, shape='box')\n",
    "    agents.node('A2', 'Agent 2\\n(System Maintenance)', style='filled', fillcolor=agent_color, shape='box')\n",
    "    agents.node('A3', 'Agent 3\\n(Operations & Automation)', style='filled', fillcolor=agent_color, shape='box')\n",
    "    agents.node('A4', 'Agent 4\\n(Monitoring & Alerts)', style='filled', fillcolor=agent_color, shape='box')\n",
    "\n",
    "# Subgraph for Agent 1 Tools (Only showing one agent's tools as an example)\n",
    "with dot.subgraph(name='cluster_tools_A1') as tools_A1:\n",
    "    tools_A1.attr(label='Agent 1 Tools', color='blue')\n",
    "    tools_A1.node('T1_A1', 'Tool 1\\n(Data Analysis)', style='filled', fillcolor=tool_color, shape='ellipse')\n",
    "    tools_A1.node('T2_A1', 'Tool 2\\n(Data Cleanup)', style='filled', fillcolor=tool_color, shape='ellipse')\n",
    "    tools_A1.node('T3_A1', 'Tool 3\\n(Visualization)', style='filled', fillcolor=tool_color, shape='ellipse')\n",
    "    tools_A1.node('T4_A1', 'Tool 4\\n(API Connection)', style='filled', fillcolor=tool_color, shape='ellipse')\n",
    "    tools_A1.node('T5_A1', 'Tool 5\\n(Reporting)', style='filled', fillcolor=tool_color, shape='ellipse')\n",
    "\n",
    "# Connecting Supervisor to Agents\n",
    "dot.edge('S', 'A1')\n",
    "dot.edge('S', 'A2')\n",
    "dot.edge('S', 'A3')\n",
    "dot.edge('S', 'A4')\n",
    "\n",
    "# Connecting Agent 1 to its Tools\n",
    "dot.edge('A1', 'T1_A1')\n",
    "dot.edge('A1', 'T2_A1')\n",
    "dot.edge('A1', 'T3_A1')\n",
    "dot.edge('A1', 'T4_A1')\n",
    "dot.edge('A1', 'T5_A1')\n",
    "\n",
    "# Render and view the graph\n",
    "dot.render('crew_ai_agents_with_subgraph', format='pdf', view=True)\n",
    "\n",
    "# Print the source for debugging\n",
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph with labels and colors\n",
    "dot = Digraph('CrewAI_Agent_Structure')\n",
    "\n",
    "# Set the colors for agents and tools\n",
    "agent_color = '#ffcc00'  # Yellow\n",
    "tool_color = '#a2d5f2'  # Light Blue\n",
    "supervisor_color = '#dff0d8'  # Light Green\n",
    "\n",
    "# Supervisor Agent\n",
    "dot.node('S', 'Supervisor Agent\\n(Manages and supervises all agents)', style='filled', fillcolor=supervisor_color, shape='box')\n",
    "\n",
    "# Agents with descriptions\n",
    "dot.node('A1', 'Agent 1\\n(Data Analysis)', style='filled', fillcolor=agent_color, shape='box')\n",
    "dot.node('A2', 'Agent 2\\n(System Maintenance)', style='filled', fillcolor=agent_color, shape='box')\n",
    "dot.node('A3', 'Agent 3\\n(Operations & Automation)', style='filled', fillcolor=agent_color, shape='box')\n",
    "dot.node('A4', 'Agent 4\\n(Monitoring & Alerts)', style='filled', fillcolor=agent_color, shape='box')\n",
    "\n",
    "# Tools for Agent 1\n",
    "dot.node('T1_A1', 'Tool 1\\n(Data Analysis)', style='filled', fillcolor=tool_color, shape='ellipse')\n",
    "dot.node('T2_A1', 'Tool 2\\n(Data Cleanup)', style='filled', fillcolor=tool_color, shape='ellipse')\n",
    "dot.node('T3_A1', 'Tool 3\\n(Visualization)', style='filled', fillcolor=tool_color, shape='ellipse')\n",
    "dot.node('T4_A1', 'Tool 4\\n(API Connection)', style='filled', fillcolor=tool_color, shape='ellipse')\n",
    "dot.node('T5_A1', 'Tool 5\\n(Reporting)', style='filled', fillcolor=tool_color, shape='ellipse')\n",
    "\n",
    "# Tools for Agent 2\n",
    "dot.node('T1_A2', 'Tool 1\\n(Monitoring)', style='filled', fillcolor=tool_color, shape='ellipse')\n",
    "dot.node('T2_A2', 'Tool 2\\n(Error Handling)', style='filled', fillcolor=tool_color, shape='ellipse')\n",
    "dot.node('T3_A2', 'Tool 3\\n(Cloud Sync)', style='filled', fillcolor=tool_color, shape='ellipse')\n",
    "dot.node('T4_A2', 'Tool 4\\n(Diagnostics)', style='filled', fillcolor=tool_color, shape='ellipse')\n",
    "dot.node('T5_A2', 'Tool 5\\n(Backup Management)', style='filled', fillcolor=tool_color, shape='ellipse')\n",
    "\n",
    "# Connecting Supervisor to Agents\n",
    "dot.edge('S', 'A1')\n",
    "dot.edge('S', 'A2')\n",
    "dot.edge('S', 'A3')\n",
    "dot.edge('S', 'A4')\n",
    "\n",
    "# Connecting Agents to Their Tools (Only Agent 1 Tools Visible by Default)\n",
    "dot.edge('A1', 'T1_A1')\n",
    "dot.edge('A1', 'T2_A1')\n",
    "dot.edge('A1', 'T3_A1')\n",
    "dot.edge('A1', 'T4_A1')\n",
    "dot.edge('A1', 'T5_A1')\n",
    "\n",
    "# Generate and display the graph\n",
    "dot.render('crew_ai_agents', format='pdf', view=True)\n",
    "\n",
    "# Print source for debugging\n",
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "# Create a directed graph\n",
    "dot = graphviz.Digraph(\"CrewAI_Workflow\", format=\"png\")\n",
    "\n",
    "# Super Agent\n",
    "dot.node(\"CrewSupervisor\", shape=\"doubleoctagon\", style=\"filled\", color=\"gold\", label=\"🧠 CrewSupervisor\")\n",
    "\n",
    "# Subgraph 1: Task Management\n",
    "with dot.subgraph(name=\"cluster_1\") as sub1:\n",
    "    sub1.attr(label=\"Task Agent\", style=\"filled\", color=\"lightgrey\")\n",
    "    sub1.node(\"TaskPlanner\", shape=\"box\")\n",
    "    sub1.node(\"TaskAllocator\", shape=\"box\")\n",
    "    sub1.edge(\"TaskPlanner\", \"TaskAllocator\")\n",
    "\n",
    "# Subgraph 2: LLM Execution\n",
    "with dot.subgraph(name=\"cluster_2\") as sub2:\n",
    "    sub2.attr(label=\"LLM Agent\", style=\"filled\", color=\"lightblue\")\n",
    "    sub2.node(\"PromptGenerator\", shape=\"box\")\n",
    "    sub2.node(\"LLMInvoker\", shape=\"box\")\n",
    "    sub2.edge(\"PromptGenerator\", \"LLMInvoker\")\n",
    "\n",
    "# Subgraph 3: Data Handling\n",
    "with dot.subgraph(name=\"cluster_3\") as sub3:\n",
    "    sub3.attr(label=\"Data Agent\", style=\"filled\", color=\"lightgreen\")\n",
    "    sub3.node(\"DatabaseManager\", shape=\"box\")\n",
    "    sub3.node(\"DataUpdater\", shape=\"box\")\n",
    "    sub3.edge(\"DatabaseManager\", \"DataUpdater\")\n",
    "\n",
    "# Subgraph 4: Monitoring Agent\n",
    "with dot.subgraph(name=\"cluster_4\") as sub4:\n",
    "    sub4.attr(label=\"Monitoring Agent\", style=\"filled\", color=\"lightcoral\")\n",
    "    sub4.node(\"Logger\", shape=\"box\")\n",
    "    sub4.node(\"AlertSystem\", shape=\"box\")\n",
    "    sub4.edge(\"Logger\", \"AlertSystem\")\n",
    "\n",
    "# Super Agent connects to all other agents\n",
    "dot.edge(\"CrewSupervisor\", \"TaskPlanner\", label=\"Manage Tasks\")\n",
    "dot.edge(\"CrewSupervisor\", \"PromptGenerator\", label=\"Control LLM\")\n",
    "dot.edge(\"CrewSupervisor\", \"DatabaseManager\", label=\"Manage Data\")\n",
    "dot.edge(\"CrewSupervisor\", \"Logger\", label=\"Monitor System\")\n",
    "\n",
    "# New Connection: TaskPlanner -> DataUpdater\n",
    "dot.edge(\"TaskPlanner\", \"DataUpdater\", label=\"Update Data\")\n",
    "\n",
    "# Render the graph\n",
    "dot.render(\"crewai_superagent_workflow\", view=True)\n",
    "\n",
    "print(\"Graph saved as 'crewai_superagent_workflow.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai_tools import ScrapeWebsiteTool\n",
    "from langchain.tools import we\n",
    "\n",
    "# To enable scrapping any website it finds during it's execution\n",
    "tool = ScrapeWebsiteTool()\n",
    "\n",
    "# Initialize the tool with the website URL, \n",
    "# so the agent can only scrap the content of the specified website\n",
    "tool = ScrapeWebsiteTool(website_url='https://www.amazon.in/dp/B0CWRZDGV1/')\n",
    "\n",
    "# Extract the text from the site\n",
    "text = tool.run()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chess knowledge base\n",
    "chess_knowledge = \"\"\"\n",
    "# Opening Principles\n",
    "1. Essential Development Rules:\n",
    "   - Don't move the same piece twice in the opening\n",
    "   - Develop knights before bishops\n",
    "   - Don't bring queen out too early\n",
    "   - Castle within the first 7-8 moves\n",
    "   - Control center with pawns (e4, d4, e5, d5)\n",
    "   - Only make pawn moves that aid development\n",
    "\n",
    "2. Common Opening Mistakes to Avoid:\n",
    "   - Moving edge pawns (a,h) too early\n",
    "   - Making too many pawn moves\n",
    "   - Moving queen prematurely\n",
    "   - Making pointless knight moves\n",
    "   - Weakening king's position\n",
    "\n",
    "3. Center Control Strategy:\n",
    "   - Occupy center with pawns first (e4/d4 or e5/d5)\n",
    "   - Support center pawns with minor pieces\n",
    "   - Don't exchange center pawns without clear benefit\n",
    "   - Maintain tension when advantageous\n",
    "\n",
    "# Middlegame Strategy\n",
    "1. King Safety Priority:\n",
    "   - Complete castling before attacking\n",
    "   - Maintain pawn shield in front of castled king\n",
    "   - Watch for diagonal weaknesses\n",
    "   - Don't advance pawns in front of castled king without purpose\n",
    "\n",
    "2. Piece Coordination:\n",
    "   - Connect rooks after castling\n",
    "   - Place bishops on active diagonals\n",
    "   - Establish knights on strong outposts\n",
    "   - Create piece chains protecting each other\n",
    "   - Coordinate pieces before launching attacks\n",
    "\n",
    "3. Attack Prerequisites:\n",
    "   - Ensure king safety first\n",
    "   - Have more pieces in attacking zone\n",
    "   - Control key squares around enemy king\n",
    "   - Create weaknesses in enemy position\n",
    "   - Don't attack without proper preparation\n",
    "\n",
    "# Position Evaluation\n",
    "1. Material Balance:\n",
    "   - Consider piece values (P=1, N=3, B=3, R=5, Q=9)\n",
    "   - Bishop pair is worth extra half-pawn\n",
    "   - Knights strong in closed positions\n",
    "   - Bishops strong in open positions\n",
    "\n",
    "2. Positional Factors:\n",
    "   - Pawn structure health\n",
    "   - Piece activity and coordination\n",
    "   - King safety assessment\n",
    "   - Control of key squares and files\n",
    "   - Development lead\n",
    "   - Space advantage\n",
    "\n",
    "3. Dynamic Elements:\n",
    "   - Piece mobility\n",
    "   - Attacking chances\n",
    "   - Tactical opportunities\n",
    "   - Pawn breaks\n",
    "   - Piece coordination potential\n",
    "\n",
    "# Common Tactical Patterns\n",
    "1. Basic Tactics:\n",
    "   - Fork: One piece attacks two\n",
    "   - Pin: Piece can't move due to exposure\n",
    "   - Skewer: Similar to pin but higher value piece in front\n",
    "   - Discovery: Moving one piece reveals attack from another\n",
    "\n",
    "2. Tactical Motifs:\n",
    "   - Overloading: Piece defending too many squares\n",
    "   - Deflection: Forcing piece away from defense\n",
    "   - Clearance: Removing blocking piece\n",
    "   - Interference: Blocking defensive piece\n",
    "\n",
    "# Safety Checks Before Moving\n",
    "1. Pre-Move Checklist:\n",
    "   - Check all opponent's captures\n",
    "   - Look for tactical threats\n",
    "   - Consider opponent's best reply\n",
    "   - Evaluate resulting position\n",
    "   - Verify move aids overall plan\n",
    "\n",
    "2. Position Maintenance:\n",
    "   - Keep pieces protected\n",
    "   - Maintain pawn structure\n",
    "   - Watch diagonal weaknesses\n",
    "   - Control key squares\n",
    "   - Keep king safe\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Chess Knowledge Base (RAG)\n",
    "def load_chess_knowledge():\n",
    "    loader = TextLoader(\"chess_knowledge.txt\")  # A document with chess theories\n",
    "    # docs = loader.load()\n",
    "    docs = [Document(page_content=chess_knowledge, metadata={\"source\": \"chess_knowledge\"})]\n",
    "    # Split the document into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    embeddings = HuggingFaceEmbeddings()\n",
    "    # vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    vectorstore = Chroma.from_documents(chunks, embeddings)\n",
    "    return vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Crew, Agent, Task\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.tools import Tool\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chess\n",
    "import chess.engine\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Load Chess Knowledge Base (RAG)\n",
    "# def load_chess_knowledge():\n",
    "#     loader = TextLoader(\"chess_knowledge.txt\")  # A document with chess theories\n",
    "#     docs = loader.load()\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "#     chunks = text_splitter.split_documents(docs)\n",
    "#     embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "#     vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "#     return vectorstore.as_retriever()\n",
    "\n",
    "retriever = load_chess_knowledge()\n",
    "\n",
    "# Load Open-Source LLM from Hugging Face\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\", \n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "# Define Chess Agents\n",
    "engine = chess.engine.SimpleEngine.popen_uci(\"stockfish\")\n",
    "\n",
    "def get_best_move(fen):\n",
    "    board = chess.Board(fen)\n",
    "    result = engine.play(board, chess.engine.Limit(time=0.5))\n",
    "    return board.san(result.move)\n",
    "\n",
    "class MoveValidationTool:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def validate_move(self, fen, move):\n",
    "        board = chess.Board(fen)\n",
    "        try:\n",
    "            chess.Move.from_uci(board.parse_san(move).uci())\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "validate_tool = MoveValidationTool()\n",
    "\n",
    "# Define Tools\n",
    "stockfish_tool = Tool(name=\"Stockfish Engine\", func=get_best_move, description=\"Suggest best move\")\n",
    "retriever_tool = Tool(name=\"Chess RAG Retriever\", func=retriever.get_relevant_documents, description=\"Retrieve chess strategies\")\n",
    "validate_move_tool = Tool(name=\"Move Validator\", func=validate_tool.validate_move, description=\"Validate move\")\n",
    "\n",
    "# Define Agents\n",
    "move_generator = Agent(\n",
    "    name=\"Move Generator\",\n",
    "    role=\"Chess Move Selector\",\n",
    "    goal=\"Suggests the best possible move for the given chess position.\",\n",
    "    backstory=\"A chess AI trained on various strategies and tactics, designed to find the optimal move in any given position.\",\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "explanation_agent = Agent(\n",
    "    name=\"Move Explainer\",\n",
    "    role=\"Chess Strategy Analyst\",\n",
    "    goal=\"Explains why the chosen move is good using chess knowledge.\",\n",
    "    backstory=\"An expert in chess theories, capable of retrieving and explaining moves using a vast knowledge base of chess strategies.\",\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "validation_agent = Agent(\n",
    "    name=\"Move Validator\",\n",
    "    role=\"Chess Move Validator\",\n",
    "    goal=\"Checks if the suggested move is valid.\",\n",
    "    backstory=\"Ensures that every suggested move follows the official rules of chess, preventing illegal or incorrect moves.\",\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Define Crew Workflow\n",
    "crew = Crew(\n",
    "    agents=[move_generator, explanation_agent, validation_agent],\n",
    "    tasks=[\n",
    "        Task(\n",
    "            name=\"Generate Move\", \n",
    "            agent=move_generator, \n",
    "            inputs=[\"FEN\"],\n",
    "            tools=[stockfish_tool],\n",
    "            description=\"Generates the best possible move for the given board position.\",\n",
    "            expected_output=\"A valid chess move in standard algebraic notation.\"\n",
    "        ),\n",
    "        Task(\n",
    "            name=\"Validate Move\", \n",
    "            agent=validation_agent, \n",
    "            inputs=[\"FEN\", \"Move\"],\n",
    "            tools=[validate_move_tool],\n",
    "            description=\"Validates if the generated move is legal and possible in the given position.\",\n",
    "            expected_output=\"Boolean value indicating whether the move is valid.\"\n",
    "        ),\n",
    "        Task(\n",
    "            name=\"Explain Move\", \n",
    "            agent=explanation_agent, \n",
    "            inputs=[\"Move\"],\n",
    "            tools=[retriever_tool],\n",
    "            description=\"Provides an explanation for why the generated move is a strong choice.\",\n",
    "            expected_output=\"A textual explanation of the move's strategy and impact.\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Example Execution\n",
    "fen = \"rnbqkb1r/pppppppp/5n2/8/8/5N2/PPPPPPPP/RNBQKB1R w KQkq - 2 2\"  # Example board state\n",
    "print(\"Current Position (FEN):\", fen)\n",
    "\n",
    "response = crew.kickoff(inputs={\"FEN\": fen})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<think>\\nOkay, so I\\'m trying to figure out how to set up an MLOps auto-deployment system using a multi-agent approach. The user wants me to generate a JSON list of agents and tasks needed for this project. Let me break down what each part requires.\\n\\nFirst, the Senior Technical Architect role is about automating MLOps, so I need to identify the main areas where automation would benefit. That probably includes model deployment, monitoring, logging, data processing, etc. So, maybe I can have agents that handle each of these tasks.\\n\\nEach agent should have a role, goal, backstory, tools used, memory capacity, verbose level, whether delegation is allowed, and caching capabilities. Let me think about what roles are essential for MLOps auto-deployment:\\n\\n1. **Model Deployer**: This would be responsible for deploying models into production environments. It needs to handle configurations, maybe scale models based on traffic, and ensure reliability.\\n\\n2. **Data Mapper**: Ensures data is compatible with deployed models by transforming it as needed. This is crucial because not all data might be in the right format or size.\\n\\n3. **Monitorer**: Continuously monitors model performance, logs metrics, and alerts if something goes wrong. It should also retrain models if performance degrades.\\n\\n4. **Logger**: Records events and metadata for transparency and auditing purposes. It should structure data for analysis later.\\n\\n5. **Analyzer**: Performs root cause analysis on model failures or system issues. This helps in understanding what went wrong and how to fix it.\\n\\n6. **Retrainer**: Automates retraining models when needed, based on feedback from monitoring or user requests.\\n\\n7. **Scaler**: Adjusts the number of deployed models up or down based on current demand to optimize resource usage.\\n\\nNow, for each agent\\'s details:\\n\\n- **Model Deployer**: Role is clear, goal is automated deployment with scalability and reliability. Backstory might be that manual deployment is error-prone and time-consuming. Tools would include CI/CD pipelines, monitoring tools, configuration management tools. Memory could store model versions and deployment logs. Verbose level should be medium for detailed logs. Delegation allowed to other agents like the Retrainer or Monitorer. Caching is important here so that deployment steps aren\\'t retried unnecessarily.\\n\\n- **Data Mapper**: Its goal is data transformation. Backstory might involve incompatible data formats causing issues. Tools would include data transformation frameworks and APIs. Memory stores mapping rules and recent transformations. Verbose is low since it\\'s more about configuration. Delegation to tools like Python scripts or Spark for heavy lifting.\\n\\n- **Monitorer**: Focuses on real-time monitoring. It needs tools like Prometheus, Grafana, and logging systems. Memory includes metrics data and alerts. Verbose should be high for detailed logs. Delegation happens when issues arise, notifying other agents like the Retrainer or Logger.\\n\\n- **Logger**: Records events. Tools are logging frameworks and APIs. Memory is log files and metadata. Verbose is medium to include timestamps and severity levels. Delegation not really needed since it\\'s a passive role but might inform others via alerts.\\n\\n- **Analyzer**: Diagnoses issues. Uses tools like ML libraries for analysis, along with monitoring data. High verbose level for detailed logs. Delegates findings to other agents when problems are identified.\\n\\n- **Retrainer**: Automates retraining based on feedback or user requests. Tools include ML frameworks and version control systems. Medium verbose for logs of retrains. Delegation occurs when it\\'s determined that a model needs updating, so it notifies the Model Deployer.\\n\\n- **Scaler**: Adjusts model counts based on load. Tools are load balancers and resource managers. High verbose for performance metrics. Delegates scaling decisions to avoid overloading resources.\\n\\nWait, but maybe I should include more roles. What about the Data Cleaner or the Configurator? Or perhaps a Resource Manager?\\n\\nAlternatively, perhaps the Model Checker is needed to validate models before deployment. That could be another agent.\\n\\nBut sticking with seven agents as per the initial list seems manageable for now. Each has specific responsibilities and tools that fit into the MLOps workflow.\\n\\nNow, for tasks: each task should have a name, description, role, tools used, expected output, and whether it\\'s saved to a file.\\n\\nFor example:\\n\\n1. **Model Validation**: Validate models before deployment to ensure they meet quality standards. Tools like validation frameworks, checklists. Output is a JSON report indicating pass/fail. File name would be model_validation_report.json.\\n\\n2. **Data Transformation**: Transform raw data into compatible formats for deployed models. Tools include APIs and transformation tools. Output is the transformed dataset saved as a CSV or parquet file.\\n\\n3. **Model Monitoring**: Collect metrics and logs, set up alerts. Tools are monitoring systems, logging tools. Outputs include metrics dashboard and alert emails.\\n\\n4. **Error Logging**: Log errors during deployment, categorize them. Tools like logging APIs, error databases. Output is an error log with categories saved as JSON in err_log.json.\\n\\n5. **Root Cause Analysis**: Analyze logs and metrics to find issues. Tools are analytics tools, debugging frameworks. Output is a report with findings and recommendations.\\n\\n6. **Model Retrain Request**: Based on performance data, decide if retraining is needed. Tools include monitoring data, ML frameworks. Outputs a JSON request for retraining.\\n\\n7. **Scale Models**: Adjust model counts based on current load. Tools like Kubernetes, resource managers. Output is the new scaling configuration saved as deployment_config.json.\\n\\nI think that covers most aspects of MLOps auto-deployment with multi-agent tasks. Each agent handles a specific part, and tasks are the actions they perform to achieve their goals. The JSON structure should reflect this hierarchy clearly.\\n</think>\\n\\n```json\\n{\\n  \"agents\": [\\n    {\\n      \"role\": \"Model Deployer\",\\n      \"goal\": \"Automate the deployment of machine learning models into production environments with scalability and reliability.\",\\n      \"backstory\": \"Manual model deployment is error-prone, time-consuming, and lacks scalability. We need an automated system that can handle retraining, scaling, and monitoring.\",\\n      \"tools\": [\"CI/CD pipelines\", \"monitoring tools\", \"configuration management tools\"],\\n      \"memory\": 1000,\\n      \"verbose\": 2,\\n      \"allow delegation\": true,\\n      \"cache\": 500\\n    },\\n    {\\n      \"role\": \"Data Mapper\",\\n      \"goal\": \"Ensure compatibility of raw data with deployed machine learning models through transformation and normalization.\",\\n      \"backstory\": \"Incompatible data formats often cause deployment failures. We need an automated data transformation system that adapts data to model requirements.\",\\n      \"tools\": [\"data transformation frameworks\", \"APIs\"],\\n      \"memory\": 500,\\n      \"verbose\": 1,\\n      \"allow delegation\": true,\\n      \"cache\": 200\\n    },\\n    {\\n      \"role\": \"Monitorer\",\\n      \"goal\": \"Continuous monitoring of deployed models to ensure performance and reliability.\",\\n      \"backstory\": \"Real-time monitoring is essential for maintaining model performance. We need a system that tracks metrics, logs, and alerts on issues.\",\\n      \"tools\": [\"Prometheus\", \"Grafana\", \"logging systems\"],\\n      \"memory\": 1000,\\n      \"verbose\": 3,\\n      \"allow delegation\": true,\\n      \"cache\": 500\\n    },\\n    {\\n      \"role\": \"Logger\",\\n      \"goal\": \"Record and store operational data for transparency, auditing, and analysis.\",\\n      \"backstory\": \"Transparency in operations is crucial. We need a robust logging system that captures all relevant events and metadata.\",\\n      \"tools\": [\"logging frameworks\", \"APIs\"],\\n      \"memory\": 500,\\n      \"verbose\": 2,\\n      \"allow delegation\": false,\\n      \"cache\": 200\\n    },\\n    {\\n      \"role\": \"Analyzer\",\\n      \"goal\": \"Perform root cause analysis on model and system issues to identify and resolve problems.\",\\n      \"backstory\": \"Understanding the root causes of failures is critical. We need an analytics system that can interpret logs, metrics, and performance data.\",\\n      \"tools\": [\"ML libraries\", \"debugging frameworks\"],\\n      \"memory\": 1000,\\n      \"verbose\": 3,\\n      \"allow delegation\": true,\\n      \"cache\": 500\\n    },\\n    {\\n      \"role\": \"Retrainer\",\\n      \"goal\": \"Automate retraining of machine learning models based on performance feedback.\",\\n      \"backstory\": \"Models may need to be retrained when their performance degrades. We need an automated system to initiate and manage retrains.\",\\n      \"tools\": [\"ML frameworks\", \"version control systems\"],\\n      \"memory\": 500,\\n      \"verbose\": 2,\\n      \"allow delegation\": true,\\n      \"cache\": 200\\n    },\\n    {\\n      \"role\": \"Scaler\",\\n      \"goal\": \"Adjust the number of deployed models based on current system load.\",\\n      \"backstory\": \"Resource optimization is essential. We need an automated scaling mechanism that adjusts model counts according to demand.\",\\n      \"tools\": [\"Kubernetes\", \"resource managers\"],\\n      \"memory\": 1000,\\n      \"verbose\": 1,\\n      \"allow delegation\": true,\\n      \"cache\": 500\\n    }\\n  ],\\n  \"tasks\": [\\n    {\\n      \"name\": \"Model Validation\",\\n      \"description\": \"Validate models before deployment to ensure they meet quality standards.\",\\n      \"tools_used\": [\"validation frameworks\", \"checklists\"],\\n      \"expected_output\": \"JSON report indicating pass/fail status for each model.\",\\n      \"file_name\": \"model_validation_report.json\"\\n    },\\n    {\\n      \"name\": \"Data Transformation\",\\n      \"description\": \"Transform raw data into compatible formats for deployed models.\",\\n      \"tools_used\": [\"data transformation tools\", \"APIs\"],\\n      \"expected_output\": \"CSV or parquet file containing the transformed dataset.\",\\n      \"file_name\": \"transformed_data.json\"\\n    },\\n    {\\n      \"name\": \"Model Monitoring\",\\n      \"description\": \"Collect metrics and logs, set up alerts for monitoring.\",\\n      \"tools_used\": [\"Prometheus\", \"Grafana\", \"logging systems\"],\\n      \"expected_output\": \"Metrics dashboard and alert emails sent to stakeholders.\",\\n      \"file_name\": \"monitoring_summary.json\"\\n    },\\n    {\\n      \"name\": \"Error Logging\",\\n      \"description\": \"Log errors during deployment and categorize them for analysis.\",\\n      \"tools_used\": [\"error logging APIs\", \"error databases\"],\\n      \"expected_output\": \"JSON error log file with categorized errors.\",\\n      \"file_name\": \"err_log.json\"\\n    },\\n    {\\n      \"name\": \"Root Cause Analysis\",\\n      \"description\": \"Analyze logs and metrics to identify issues.\",\\n      \"tools_used\": [\"analytics tools\", \"debugging frameworks\"],\\n      \"expected_output\": \"Report detailing findings and recommendations for resolution.\",\\n      \"file_name\": \"analysis_report.json\"\\n    },\\n    {\\n      \"name\": \"Model Retrain Request\",\\n      \"description\": \"Generate a request to retrain models based on performance data.\",\\n      \"tools_used\": [\"monitoring data\", \"ML frameworks\"],\\n      \"expected_output\": \"JSON request for model retraining.\",\\n      \"file_name\": \"retrain_request.json\"\\n    },\\n    {\\n      \"name\": \"Scale Models\",\\n      \"description\": \"Adjust model counts based on current system load.\",\\n      \"tools_used\": [\"Kubernetes\", \"resource managers\"],\\n      \"expected_output\": \"New scaling configuration file to be applied in production environment.\",\\n      \"file_name\": \"deployment_config.json\"\\n    }\\n  ]\\n}\\n```' additional_kwargs={} response_metadata={'model': 'deepseek-r1:8b', 'created_at': '2025-02-13T05:37:46.774342Z', 'done': True, 'done_reason': 'stop', 'total_duration': 137095413917, 'load_duration': 1104064209, 'prompt_eval_count': 165, 'prompt_eval_duration': 3056000000, 'eval_count': 2400, 'eval_duration': 132932000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-4e5973e2-ac45-41d2-b797-c66c864d40e5-0' usage_metadata={'input_tokens': 165, 'output_tokens': 2400, 'total_tokens': 2565}\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task\n",
    "from langchain.llms import Ollama\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import HumanMessage\n",
    "import json\n",
    "\n",
    "class PlannerAgent:\n",
    "    def __init__(self):\n",
    "        # model_name = \"llama-3.2\"\n",
    "        model_name = \"deepseek-r1:8b\"\n",
    "        # model_name = \"llama3:latest\"\n",
    "        self.llm = ChatOllama(model=model_name)\n",
    "        # self.llm = Ollama(model=\"llama3\")\n",
    "        \n",
    "    def generate_plan(self):\n",
    "        prompt = \"\"\"\n",
    "        You are a Senior Technical Architect responsible for automating MLOps auto-deployment using a multi-agent system.\n",
    "        Generate a JSON list of agents and tasks needed for this project.\n",
    "        \n",
    "        Each agent should have:\n",
    "        - role\n",
    "        - goal\n",
    "        - backstory\n",
    "        - tools\n",
    "        - memory\n",
    "        - verbose\n",
    "        - allow delegation\n",
    "        - cache\n",
    "        \n",
    "        Each task should have:\n",
    "        - name\n",
    "        - description\n",
    "        - agent role\n",
    "        - tools\n",
    "        - expected output\n",
    "        - output JSON\n",
    "        - output file name\n",
    "\n",
    "        output format: json\n",
    "        {agents: [{\n",
    "        role:,goal:,backstory,tools,memory,verbose,allow delegation,cache}], tasks: [{task1}, {task2}, ...]\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        # response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "        response = self.llm([HumanMessage(content=prompt)])\n",
    "        # plan = json.loads(response.content)\n",
    "        # return plan\n",
    "        return response\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    planner = PlannerAgent()\n",
    "    plan = planner.generate_plan()\n",
    "    # print(json.dumps(plan, indent=4))\n",
    "    print(plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, so I\\'m trying to figure out how to set up an MLOps auto-deployment system using a multi-agent approach. The user wants me to generate a JSON list of agents and tasks needed for this project. Let me break down what each part requires.\\n\\nFirst, the Senior Technical Architect role is about automating MLOps, so I need to identify the main areas where automation would benefit. That probably includes model deployment, monitoring, logging, data processing, etc. So, maybe I can have agents that handle each of these tasks.\\n\\nEach agent should have a role, goal, backstory, tools used, memory capacity, verbose level, whether delegation is allowed, and caching capabilities. Let me think about what roles are essential for MLOps auto-deployment:\\n\\n1. **Model Deployer**: This would be responsible for deploying models into production environments. It needs to handle configurations, maybe scale models based on traffic, and ensure reliability.\\n\\n2. **Data Mapper**: Ensures data is compatible with deployed models by transforming it as needed. This is crucial because not all data might be in the right format or size.\\n\\n3. **Monitorer**: Continuously monitors model performance, logs metrics, and alerts if something goes wrong. It should also retrain models if performance degrades.\\n\\n4. **Logger**: Records events and metadata for transparency and auditing purposes. It should structure data for analysis later.\\n\\n5. **Analyzer**: Performs root cause analysis on model failures or system issues. This helps in understanding what went wrong and how to fix it.\\n\\n6. **Retrainer**: Automates retraining models when needed, based on feedback from monitoring or user requests.\\n\\n7. **Scaler**: Adjusts the number of deployed models up or down based on current demand to optimize resource usage.\\n\\nNow, for each agent\\'s details:\\n\\n- **Model Deployer**: Role is clear, goal is automated deployment with scalability and reliability. Backstory might be that manual deployment is error-prone and time-consuming. Tools would include CI/CD pipelines, monitoring tools, configuration management tools. Memory could store model versions and deployment logs. Verbose level should be medium for detailed logs. Delegation allowed to other agents like the Retrainer or Monitorer. Caching is important here so that deployment steps aren\\'t retried unnecessarily.\\n\\n- **Data Mapper**: Its goal is data transformation. Backstory might involve incompatible data formats causing issues. Tools would include data transformation frameworks and APIs. Memory stores mapping rules and recent transformations. Verbose is low since it\\'s more about configuration. Delegation to tools like Python scripts or Spark for heavy lifting.\\n\\n- **Monitorer**: Focuses on real-time monitoring. It needs tools like Prometheus, Grafana, and logging systems. Memory includes metrics data and alerts. Verbose should be high for detailed logs. Delegation happens when issues arise, notifying other agents like the Retrainer or Logger.\\n\\n- **Logger**: Records events. Tools are logging frameworks and APIs. Memory is log files and metadata. Verbose is medium to include timestamps and severity levels. Delegation not really needed since it\\'s a passive role but might inform others via alerts.\\n\\n- **Analyzer**: Diagnoses issues. Uses tools like ML libraries for analysis, along with monitoring data. High verbose level for detailed logs. Delegates findings to other agents when problems are identified.\\n\\n- **Retrainer**: Automates retraining based on feedback or user requests. Tools include ML frameworks and version control systems. Medium verbose for logs of retrains. Delegation occurs when it\\'s determined that a model needs updating, so it notifies the Model Deployer.\\n\\n- **Scaler**: Adjusts model counts based on load. Tools are load balancers and resource managers. High verbose for performance metrics. Delegates scaling decisions to avoid overloading resources.\\n\\nWait, but maybe I should include more roles. What about the Data Cleaner or the Configurator? Or perhaps a Resource Manager?\\n\\nAlternatively, perhaps the Model Checker is needed to validate models before deployment. That could be another agent.\\n\\nBut sticking with seven agents as per the initial list seems manageable for now. Each has specific responsibilities and tools that fit into the MLOps workflow.\\n\\nNow, for tasks: each task should have a name, description, role, tools used, expected output, and whether it\\'s saved to a file.\\n\\nFor example:\\n\\n1. **Model Validation**: Validate models before deployment to ensure they meet quality standards. Tools like validation frameworks, checklists. Output is a JSON report indicating pass/fail. File name would be model_validation_report.json.\\n\\n2. **Data Transformation**: Transform raw data into compatible formats for deployed models. Tools include APIs and transformation tools. Output is the transformed dataset saved as a CSV or parquet file.\\n\\n3. **Model Monitoring**: Collect metrics and logs, set up alerts. Tools are monitoring systems, logging tools. Outputs include metrics dashboard and alert emails.\\n\\n4. **Error Logging**: Log errors during deployment, categorize them. Tools like logging APIs, error databases. Output is an error log with categories saved as JSON in err_log.json.\\n\\n5. **Root Cause Analysis**: Analyze logs and metrics to find issues. Tools are analytics tools, debugging frameworks. Output is a report with findings and recommendations.\\n\\n6. **Model Retrain Request**: Based on performance data, decide if retraining is needed. Tools include monitoring data, ML frameworks. Outputs a JSON request for retraining.\\n\\n7. **Scale Models**: Adjust model counts based on current load. Tools like Kubernetes, resource managers. Output is the new scaling configuration saved as deployment_config.json.\\n\\nI think that covers most aspects of MLOps auto-deployment with multi-agent tasks. Each agent handles a specific part, and tasks are the actions they perform to achieve their goals. The JSON structure should reflect this hierarchy clearly.\\n</think>\\n\\n```json\\n{\\n  \"agents\": [\\n    {\\n      \"role\": \"Model Deployer\",\\n      \"goal\": \"Automate the deployment of machine learning models into production environments with scalability and reliability.\",\\n      \"backstory\": \"Manual model deployment is error-prone, time-consuming, and lacks scalability. We need an automated system that can handle retraining, scaling, and monitoring.\",\\n      \"tools\": [\"CI/CD pipelines\", \"monitoring tools\", \"configuration management tools\"],\\n      \"memory\": 1000,\\n      \"verbose\": 2,\\n      \"allow delegation\": true,\\n      \"cache\": 500\\n    },\\n    {\\n      \"role\": \"Data Mapper\",\\n      \"goal\": \"Ensure compatibility of raw data with deployed machine learning models through transformation and normalization.\",\\n      \"backstory\": \"Incompatible data formats often cause deployment failures. We need an automated data transformation system that adapts data to model requirements.\",\\n      \"tools\": [\"data transformation frameworks\", \"APIs\"],\\n      \"memory\": 500,\\n      \"verbose\": 1,\\n      \"allow delegation\": true,\\n      \"cache\": 200\\n    },\\n    {\\n      \"role\": \"Monitorer\",\\n      \"goal\": \"Continuous monitoring of deployed models to ensure performance and reliability.\",\\n      \"backstory\": \"Real-time monitoring is essential for maintaining model performance. We need a system that tracks metrics, logs, and alerts on issues.\",\\n      \"tools\": [\"Prometheus\", \"Grafana\", \"logging systems\"],\\n      \"memory\": 1000,\\n      \"verbose\": 3,\\n      \"allow delegation\": true,\\n      \"cache\": 500\\n    },\\n    {\\n      \"role\": \"Logger\",\\n      \"goal\": \"Record and store operational data for transparency, auditing, and analysis.\",\\n      \"backstory\": \"Transparency in operations is crucial. We need a robust logging system that captures all relevant events and metadata.\",\\n      \"tools\": [\"logging frameworks\", \"APIs\"],\\n      \"memory\": 500,\\n      \"verbose\": 2,\\n      \"allow delegation\": false,\\n      \"cache\": 200\\n    },\\n    {\\n      \"role\": \"Analyzer\",\\n      \"goal\": \"Perform root cause analysis on model and system issues to identify and resolve problems.\",\\n      \"backstory\": \"Understanding the root causes of failures is critical. We need an analytics system that can interpret logs, metrics, and performance data.\",\\n      \"tools\": [\"ML libraries\", \"debugging frameworks\"],\\n      \"memory\": 1000,\\n      \"verbose\": 3,\\n      \"allow delegation\": true,\\n      \"cache\": 500\\n    },\\n    {\\n      \"role\": \"Retrainer\",\\n      \"goal\": \"Automate retraining of machine learning models based on performance feedback.\",\\n      \"backstory\": \"Models may need to be retrained when their performance degrades. We need an automated system to initiate and manage retrains.\",\\n      \"tools\": [\"ML frameworks\", \"version control systems\"],\\n      \"memory\": 500,\\n      \"verbose\": 2,\\n      \"allow delegation\": true,\\n      \"cache\": 200\\n    },\\n    {\\n      \"role\": \"Scaler\",\\n      \"goal\": \"Adjust the number of deployed models based on current system load.\",\\n      \"backstory\": \"Resource optimization is essential. We need an automated scaling mechanism that adjusts model counts according to demand.\",\\n      \"tools\": [\"Kubernetes\", \"resource managers\"],\\n      \"memory\": 1000,\\n      \"verbose\": 1,\\n      \"allow delegation\": true,\\n      \"cache\": 500\\n    }\\n  ],\\n  \"tasks\": [\\n    {\\n      \"name\": \"Model Validation\",\\n      \"description\": \"Validate models before deployment to ensure they meet quality standards.\",\\n      \"tools_used\": [\"validation frameworks\", \"checklists\"],\\n      \"expected_output\": \"JSON report indicating pass/fail status for each model.\",\\n      \"file_name\": \"model_validation_report.json\"\\n    },\\n    {\\n      \"name\": \"Data Transformation\",\\n      \"description\": \"Transform raw data into compatible formats for deployed models.\",\\n      \"tools_used\": [\"data transformation tools\", \"APIs\"],\\n      \"expected_output\": \"CSV or parquet file containing the transformed dataset.\",\\n      \"file_name\": \"transformed_data.json\"\\n    },\\n    {\\n      \"name\": \"Model Monitoring\",\\n      \"description\": \"Collect metrics and logs, set up alerts for monitoring.\",\\n      \"tools_used\": [\"Prometheus\", \"Grafana\", \"logging systems\"],\\n      \"expected_output\": \"Metrics dashboard and alert emails sent to stakeholders.\",\\n      \"file_name\": \"monitoring_summary.json\"\\n    },\\n    {\\n      \"name\": \"Error Logging\",\\n      \"description\": \"Log errors during deployment and categorize them for analysis.\",\\n      \"tools_used\": [\"error logging APIs\", \"error databases\"],\\n      \"expected_output\": \"JSON error log file with categorized errors.\",\\n      \"file_name\": \"err_log.json\"\\n    },\\n    {\\n      \"name\": \"Root Cause Analysis\",\\n      \"description\": \"Analyze logs and metrics to identify issues.\",\\n      \"tools_used\": [\"analytics tools\", \"debugging frameworks\"],\\n      \"expected_output\": \"Report detailing findings and recommendations for resolution.\",\\n      \"file_name\": \"analysis_report.json\"\\n    },\\n    {\\n      \"name\": \"Model Retrain Request\",\\n      \"description\": \"Generate a request to retrain models based on performance data.\",\\n      \"tools_used\": [\"monitoring data\", \"ML frameworks\"],\\n      \"expected_output\": \"JSON request for model retraining.\",\\n      \"file_name\": \"retrain_request.json\"\\n    },\\n    {\\n      \"name\": \"Scale Models\",\\n      \"description\": \"Adjust model counts based on current system load.\",\\n      \"tools_used\": [\"Kubernetes\", \"resource managers\"],\\n      \"expected_output\": \"New scaling configuration file to be applied in production environment.\",\\n      \"file_name\": \"deployment_config.json\"\\n    }\\n  ]\\n}\\n```', additional_kwargs={}, response_metadata={'model': 'deepseek-r1:8b', 'created_at': '2025-02-13T05:37:46.774342Z', 'done': True, 'done_reason': 'stop', 'total_duration': 137095413917, 'load_duration': 1104064209, 'prompt_eval_count': 165, 'prompt_eval_duration': 3056000000, 'eval_count': 2400, 'eval_duration': 132932000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-4e5973e2-ac45-41d2-b797-c66c864d40e5-0', usage_metadata={'input_tokens': 165, 'output_tokens': 2400, 'total_tokens': 2565})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out how to set up an MLOps auto-deployment system using a multi-agent approach. The user wants me to generate a JSON list of agents and tasks needed for this project. Let me break down what each part requires.\n",
      "\n",
      "First, the Senior Technical Architect role is about automating MLOps, so I need to identify the main areas where automation would benefit. That probably includes model deployment, monitoring, logging, data processing, etc. So, maybe I can have agents that handle each of these tasks.\n",
      "\n",
      "Each agent should have a role, goal, backstory, tools used, memory capacity, verbose level, whether delegation is allowed, and caching capabilities. Let me think about what roles are essential for MLOps auto-deployment:\n",
      "\n",
      "1. **Model Deployer**: This would be responsible for deploying models into production environments. It needs to handle configurations, maybe scale models based on traffic, and ensure reliability.\n",
      "\n",
      "2. **Data Mapper**: Ensures data is compatible with deployed models by transforming it as needed. This is crucial because not all data might be in the right format or size.\n",
      "\n",
      "3. **Monitorer**: Continuously monitors model performance, logs metrics, and alerts if something goes wrong. It should also retrain models if performance degrades.\n",
      "\n",
      "4. **Logger**: Records events and metadata for transparency and auditing purposes. It should structure data for analysis later.\n",
      "\n",
      "5. **Analyzer**: Performs root cause analysis on model failures or system issues. This helps in understanding what went wrong and how to fix it.\n",
      "\n",
      "6. **Retrainer**: Automates retraining models when needed, based on feedback from monitoring or user requests.\n",
      "\n",
      "7. **Scaler**: Adjusts the number of deployed models up or down based on current demand to optimize resource usage.\n",
      "\n",
      "Now, for each agent's details:\n",
      "\n",
      "- **Model Deployer**: Role is clear, goal is automated deployment with scalability and reliability. Backstory might be that manual deployment is error-prone and time-consuming. Tools would include CI/CD pipelines, monitoring tools, configuration management tools. Memory could store model versions and deployment logs. Verbose level should be medium for detailed logs. Delegation allowed to other agents like the Retrainer or Monitorer. Caching is important here so that deployment steps aren't retried unnecessarily.\n",
      "\n",
      "- **Data Mapper**: Its goal is data transformation. Backstory might involve incompatible data formats causing issues. Tools would include data transformation frameworks and APIs. Memory stores mapping rules and recent transformations. Verbose is low since it's more about configuration. Delegation to tools like Python scripts or Spark for heavy lifting.\n",
      "\n",
      "- **Monitorer**: Focuses on real-time monitoring. It needs tools like Prometheus, Grafana, and logging systems. Memory includes metrics data and alerts. Verbose should be high for detailed logs. Delegation happens when issues arise, notifying other agents like the Retrainer or Logger.\n",
      "\n",
      "- **Logger**: Records events. Tools are logging frameworks and APIs. Memory is log files and metadata. Verbose is medium to include timestamps and severity levels. Delegation not really needed since it's a passive role but might inform others via alerts.\n",
      "\n",
      "- **Analyzer**: Diagnoses issues. Uses tools like ML libraries for analysis, along with monitoring data. High verbose level for detailed logs. Delegates findings to other agents when problems are identified.\n",
      "\n",
      "- **Retrainer**: Automates retraining based on feedback or user requests. Tools include ML frameworks and version control systems. Medium verbose for logs of retrains. Delegation occurs when it's determined that a model needs updating, so it notifies the Model Deployer.\n",
      "\n",
      "- **Scaler**: Adjusts model counts based on load. Tools are load balancers and resource managers. High verbose for performance metrics. Delegates scaling decisions to avoid overloading resources.\n",
      "\n",
      "Wait, but maybe I should include more roles. What about the Data Cleaner or the Configurator? Or perhaps a Resource Manager?\n",
      "\n",
      "Alternatively, perhaps the Model Checker is needed to validate models before deployment. That could be another agent.\n",
      "\n",
      "But sticking with seven agents as per the initial list seems manageable for now. Each has specific responsibilities and tools that fit into the MLOps workflow.\n",
      "\n",
      "Now, for tasks: each task should have a name, description, role, tools used, expected output, and whether it's saved to a file.\n",
      "\n",
      "For example:\n",
      "\n",
      "1. **Model Validation**: Validate models before deployment to ensure they meet quality standards. Tools like validation frameworks, checklists. Output is a JSON report indicating pass/fail. File name would be model_validation_report.json.\n",
      "\n",
      "2. **Data Transformation**: Transform raw data into compatible formats for deployed models. Tools include APIs and transformation tools. Output is the transformed dataset saved as a CSV or parquet file.\n",
      "\n",
      "3. **Model Monitoring**: Collect metrics and logs, set up alerts. Tools are monitoring systems, logging tools. Outputs include metrics dashboard and alert emails.\n",
      "\n",
      "4. **Error Logging**: Log errors during deployment, categorize them. Tools like logging APIs, error databases. Output is an error log with categories saved as JSON in err_log.json.\n",
      "\n",
      "5. **Root Cause Analysis**: Analyze logs and metrics to find issues. Tools are analytics tools, debugging frameworks. Output is a report with findings and recommendations.\n",
      "\n",
      "6. **Model Retrain Request**: Based on performance data, decide if retraining is needed. Tools include monitoring data, ML frameworks. Outputs a JSON request for retraining.\n",
      "\n",
      "7. **Scale Models**: Adjust model counts based on current load. Tools like Kubernetes, resource managers. Output is the new scaling configuration saved as deployment_config.json.\n",
      "\n",
      "I think that covers most aspects of MLOps auto-deployment with multi-agent tasks. Each agent handles a specific part, and tasks are the actions they perform to achieve their goals. The JSON structure should reflect this hierarchy clearly.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"agents\": [\n",
      "    {\n",
      "      \"role\": \"Model Deployer\",\n",
      "      \"goal\": \"Automate the deployment of machine learning models into production environments with scalability and reliability.\",\n",
      "      \"backstory\": \"Manual model deployment is error-prone, time-consuming, and lacks scalability. We need an automated system that can handle retraining, scaling, and monitoring.\",\n",
      "      \"tools\": [\"CI/CD pipelines\", \"monitoring tools\", \"configuration management tools\"],\n",
      "      \"memory\": 1000,\n",
      "      \"verbose\": 2,\n",
      "      \"allow delegation\": true,\n",
      "      \"cache\": 500\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"Data Mapper\",\n",
      "      \"goal\": \"Ensure compatibility of raw data with deployed machine learning models through transformation and normalization.\",\n",
      "      \"backstory\": \"Incompatible data formats often cause deployment failures. We need an automated data transformation system that adapts data to model requirements.\",\n",
      "      \"tools\": [\"data transformation frameworks\", \"APIs\"],\n",
      "      \"memory\": 500,\n",
      "      \"verbose\": 1,\n",
      "      \"allow delegation\": true,\n",
      "      \"cache\": 200\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"Monitorer\",\n",
      "      \"goal\": \"Continuous monitoring of deployed models to ensure performance and reliability.\",\n",
      "      \"backstory\": \"Real-time monitoring is essential for maintaining model performance. We need a system that tracks metrics, logs, and alerts on issues.\",\n",
      "      \"tools\": [\"Prometheus\", \"Grafana\", \"logging systems\"],\n",
      "      \"memory\": 1000,\n",
      "      \"verbose\": 3,\n",
      "      \"allow delegation\": true,\n",
      "      \"cache\": 500\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"Logger\",\n",
      "      \"goal\": \"Record and store operational data for transparency, auditing, and analysis.\",\n",
      "      \"backstory\": \"Transparency in operations is crucial. We need a robust logging system that captures all relevant events and metadata.\",\n",
      "      \"tools\": [\"logging frameworks\", \"APIs\"],\n",
      "      \"memory\": 500,\n",
      "      \"verbose\": 2,\n",
      "      \"allow delegation\": false,\n",
      "      \"cache\": 200\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"Analyzer\",\n",
      "      \"goal\": \"Perform root cause analysis on model and system issues to identify and resolve problems.\",\n",
      "      \"backstory\": \"Understanding the root causes of failures is critical. We need an analytics system that can interpret logs, metrics, and performance data.\",\n",
      "      \"tools\": [\"ML libraries\", \"debugging frameworks\"],\n",
      "      \"memory\": 1000,\n",
      "      \"verbose\": 3,\n",
      "      \"allow delegation\": true,\n",
      "      \"cache\": 500\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"Retrainer\",\n",
      "      \"goal\": \"Automate retraining of machine learning models based on performance feedback.\",\n",
      "      \"backstory\": \"Models may need to be retrained when their performance degrades. We need an automated system to initiate and manage retrains.\",\n",
      "      \"tools\": [\"ML frameworks\", \"version control systems\"],\n",
      "      \"memory\": 500,\n",
      "      \"verbose\": 2,\n",
      "      \"allow delegation\": true,\n",
      "      \"cache\": 200\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"Scaler\",\n",
      "      \"goal\": \"Adjust the number of deployed models based on current system load.\",\n",
      "      \"backstory\": \"Resource optimization is essential. We need an automated scaling mechanism that adjusts model counts according to demand.\",\n",
      "      \"tools\": [\"Kubernetes\", \"resource managers\"],\n",
      "      \"memory\": 1000,\n",
      "      \"verbose\": 1,\n",
      "      \"allow delegation\": true,\n",
      "      \"cache\": 500\n",
      "    }\n",
      "  ],\n",
      "  \"tasks\": [\n",
      "    {\n",
      "      \"name\": \"Model Validation\",\n",
      "      \"description\": \"Validate models before deployment to ensure they meet quality standards.\",\n",
      "      \"tools_used\": [\"validation frameworks\", \"checklists\"],\n",
      "      \"expected_output\": \"JSON report indicating pass/fail status for each model.\",\n",
      "      \"file_name\": \"model_validation_report.json\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Data Transformation\",\n",
      "      \"description\": \"Transform raw data into compatible formats for deployed models.\",\n",
      "      \"tools_used\": [\"data transformation tools\", \"APIs\"],\n",
      "      \"expected_output\": \"CSV or parquet file containing the transformed dataset.\",\n",
      "      \"file_name\": \"transformed_data.json\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Model Monitoring\",\n",
      "      \"description\": \"Collect metrics and logs, set up alerts for monitoring.\",\n",
      "      \"tools_used\": [\"Prometheus\", \"Grafana\", \"logging systems\"],\n",
      "      \"expected_output\": \"Metrics dashboard and alert emails sent to stakeholders.\",\n",
      "      \"file_name\": \"monitoring_summary.json\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Error Logging\",\n",
      "      \"description\": \"Log errors during deployment and categorize them for analysis.\",\n",
      "      \"tools_used\": [\"error logging APIs\", \"error databases\"],\n",
      "      \"expected_output\": \"JSON error log file with categorized errors.\",\n",
      "      \"file_name\": \"err_log.json\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Root Cause Analysis\",\n",
      "      \"description\": \"Analyze logs and metrics to identify issues.\",\n",
      "      \"tools_used\": [\"analytics tools\", \"debugging frameworks\"],\n",
      "      \"expected_output\": \"Report detailing findings and recommendations for resolution.\",\n",
      "      \"file_name\": \"analysis_report.json\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Model Retrain Request\",\n",
      "      \"description\": \"Generate a request to retrain models based on performance data.\",\n",
      "      \"tools_used\": [\"monitoring data\", \"ML frameworks\"],\n",
      "      \"expected_output\": \"JSON request for model retraining.\",\n",
      "      \"file_name\": \"retrain_request.json\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Scale Models\",\n",
      "      \"description\": \"Adjust model counts based on current system load.\",\n",
      "      \"tools_used\": [\"Kubernetes\", \"resource managers\"],\n",
      "      \"expected_output\": \"New scaling configuration file to be applied in production environment.\",\n",
      "      \"file_name\": \"deployment_config.json\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(plan.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plan.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a fascinating project! As a Senior Technical Architect, I'd be delighted to help you generate the JSON list of agents and tasks needed for automating MLOps auto-deployment using a multi-agent system.\n",
      "\n",
      "Here's the JSON data:\n",
      "\n",
      "**Agents**\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"role\": \"Model Trainer\",\n",
      "    \"goal\": \"Train and validate machine learning models\",\n",
      "    \"backstory\": \"Experienced in training and validating various machine learning models for classification, regression, clustering, etc.\",\n",
      "    \"tools\": [\"TensorFlow\", \"PyTorch\"],\n",
      "    \"memory\": 16,\n",
      "    \"verbose\": true,\n",
      "    \"allow_delegation\": false,\n",
      "    \"cache\": {\n",
      "      \"model_data\": {}\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"Model Deployer\",\n",
      "    \"goal\": \"Deploy trained models to production environments\",\n",
      "    \"backstory\": \"Proficient in deploying models using various frameworks such as TensorFlow Serving, AWS SageMaker, etc.\",\n",
      "    \"tools\": [\"TensorFlow Serving\", \"AWS SageMaker\"],\n",
      "    \"memory\": 8,\n",
      "    \"verbose\": false,\n",
      "    \"allow_delegation\": true,\n",
      "    \"cache\": {\n",
      "      \"model_configs\": {}\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"Data Engineer\",\n",
      "    \"goal\": \"Ingest, process, and store data for training and validation\",\n",
      "    \"backstory\": \"Skilled in working with various data storage systems such as relational databases, NoSQL databases, and data warehouses.\",\n",
      "    \"tools\": [\"Pandas\", \"Apache Spark\"],\n",
      "    \"memory\": 32,\n",
      "    \"verbose\": true,\n",
      "    \"allow_delegation\": false,\n",
      "    \"cache\": {\n",
      "      \"data_catalog\": {}\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"Model Monitor\",\n",
      "    \"goal\": \"Monitor and analyze model performance in production environments\",\n",
      "    \"backstory\": \"Experienced in monitoring and analyzing model performance using tools like Prometheus, Grafana, etc.\",\n",
      "    \"tools\": [\"Prometheus\", \"Grafana\"],\n",
      "    \"memory\": 16,\n",
      "    \"verbose\": true,\n",
      "    \"allow_delegation\": false,\n",
      "    \"cache\": {\n",
      "      \"model_metrics\": {}\n",
      "    }\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "**Tasks**\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"name\": \"Train Model\",\n",
      "    \"description\": \"Train a machine learning model using provided training data\",\n",
      "    \"agent_role\": \"Model Trainer\",\n",
      "    \"tools\": [\"TensorFlow\", \"PyTorch\"],\n",
      "    \"expected_output\": \"Trained model parameters\",\n",
      "    \"output_json\": {\"model_params\": {}},\n",
      "    \"output_file_name\": \"trained_model.json\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Deploy Model\",\n",
      "    \"description\": \"Deploy a trained machine learning model to production environment\",\n",
      "    \"agent_role\": \"Model Deployer\",\n",
      "    \"tools\": [\"TensorFlow Serving\", \"AWS SageMaker\"],\n",
      "    \"expected_output\": \"Deployment status\",\n",
      "    \"output_json\": {\"deployment_status\": {}},\n",
      "    \"output_file_name\": \"deployment_status.json\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Ingest Data\",\n",
      "    \"description\": \"Ingest data from various sources and store it in a database\",\n",
      "    \"agent_role\": \"Data Engineer\",\n",
      "    \"tools\": [\"Pandas\", \"Apache Spark\"],\n",
      "    \"expected_output\": \"Data ingestion status\",\n",
      "    \"output_json\": {\"data_ingestion_status\": {}},\n",
      "    \"output_file_name\": \"data_ingestion_status.json\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Monitor Model\",\n",
      "    \"description\": \"Monitor and analyze the performance of a deployed machine learning model\",\n",
      "    \"agent_role\": \"Model Monitor\",\n",
      "    \"tools\": [\"Prometheus\", \"Grafana\"],\n",
      "    \"expected_output\": \"Model performance metrics\",\n",
      "    \"output_json\": {\"model_performance_metrics\": {}},\n",
      "    \"output_file_name\": \"model_performance_metrics.json\"\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Please note that this is just a sample JSON data and might need to be adjusted according to your specific project requirements.\n"
     ]
    }
   ],
   "source": [
    "print(plan.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
